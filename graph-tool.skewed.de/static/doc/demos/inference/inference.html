
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Inferring modular network structure &#8212; graph-tool 2.27 documentation</title>
    <link rel="stylesheet" href="../../_static/gt_style.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/MathJax/MathJax.js?config=default"></script>
    <link rel="search" type="application/opensearchdescription+xml"
          title="Search within graph-tool 2.27 documentation"
          href="../../_static/opensearch.xml"/>
    <link rel="shortcut icon" href="../../_static/graph-icon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Animations with graph-tool" href="../animation/animation.html" />
    <link rel="prev" title="Cookbook" href="../index.html" />
    <script type="text/javascript">
      var _gaq = _gaq || [];
      var pluginUrl =
        '//www.google-analytics.com/plugins/ga/inpage_linkid.js';
      _gaq.push(['_require', 'inpage_linkid', pluginUrl]);
      _gaq.push(['_setAccount', 'UA-248813-2']);
      _gaq.push(['_setDomainName', '.skewed.de']);
      _gaq.push(['_trackPageview']);
      (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
     

  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../animation/animation.html" title="Animations with graph-tool"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../index.html" title="Cookbook"
             accesskey="P">previous</a> |</li>
    <li><img src="../../../../img/graph-icon.png" alt="logo" style="margin-right: 5px; margin-bottom:-2px;" /><a href="../../../../index.html">Project Homepage</a> &raquo;</li>
    
        <li class="nav-item nav-item-0"><a href="../../index.html">graph-tool 2.27 documentation</a> &#187;</li>

          <li class="nav-item nav-item-1"><a href="../index.html" accesskey="U">Cookbook</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="inferring-modular-network-structure">
<span id="inference-howto"></span><h1>Inferring modular network structure<a class="headerlink" href="inference.html#inferring-modular-network-structure" title="Permalink to this headline">¶</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">graph-tool</span></code> includes algorithms to identify the large-scale structure
of networks in the <a class="reference internal" href="../../inference.html#module-graph_tool.inference" title="graph_tool.inference"><code class="xref py py-mod docutils literal notranslate"><span class="pre">inference</span></code></a> submodule. Here we
explain the basic functionality with self-contained examples. For a more
thorough theoretical introduction to the methods described here, the
reader is referred to <a class="reference internal" href="inference.html#peixoto-bayesian-2017" id="id1">[peixoto-bayesian-2017]</a>.</p>
<div class="section" id="background-nonparametric-statistical-inference">
<h2>Background: Nonparametric statistical inference<a class="headerlink" href="inference.html#background-nonparametric-statistical-inference" title="Permalink to this headline">¶</a></h2>
<p>A common task when analyzing networks is to characterize their
structures in simple terms, often by dividing the nodes into modules or
<a class="reference external" href="https://en.wikipedia.org/wiki/Community_structure">“communities”</a>.</p>
<p>A principled approach to perform this task is to formulate <a class="reference external" href="https://en.wikipedia.org/wiki/Generative_model">generative
models</a> that include
the idea of “modules” in their descriptions, which then can be detected
by <a class="reference external" href="https://en.wikipedia.org/wiki/Statistical_inference">inferring</a>
the model parameters from data. More precisely, given the partition
<span class="math">\(\boldsymbol b = \{b_i\}\)</span> of the network into <span class="math">\(B\)</span> groups,
where <span class="math">\(b_i\in[0,B-1]\)</span> is the group membership of node <span class="math">\(i\)</span>,
we define a model that generates a network <span class="math">\(\boldsymbol G\)</span> with a
probability</p>
<div class="math" id="equation-model-likelihood">
<span class="eqno">(1)</span>\[P(\boldsymbol G|\boldsymbol\theta, \boldsymbol b)\]</div>
<p>where <span class="math">\(\boldsymbol\theta\)</span> are additional model parameters that
control how the node partition affects the structure of the
network. Therefore, if we observe a network <span class="math">\(\boldsymbol G\)</span>, the
likelihood that it was generated by a given partition <span class="math">\(\boldsymbol
b\)</span> is obtained via the <a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian</a> posterior probability</p>
<div class="math" id="equation-model-posterior-sum">
<span class="eqno">(2)</span>\[P(\boldsymbol b | \boldsymbol G) = \frac{\sum_{\boldsymbol\theta}P(\boldsymbol G|\boldsymbol\theta, \boldsymbol b)P(\boldsymbol\theta, \boldsymbol b)}{P(\boldsymbol G)}\]</div>
<p>where <span class="math">\(P(\boldsymbol\theta, \boldsymbol b)\)</span> is the <a class="reference external" href="https://en.wikipedia.org/wiki/Prior_probability">prior
probability</a> of the
model parameters, and</p>
<div class="math" id="equation-model-evidence">
<span class="eqno">(3)</span>\[P(\boldsymbol G) = \sum_{\boldsymbol\theta,\boldsymbol b}P(\boldsymbol G|\boldsymbol\theta, \boldsymbol b)P(\boldsymbol\theta, \boldsymbol b)\]</div>
<p>is called the <cite>evidence</cite>, and corresponds to the total probability of
the data summed over all model parameters. The particular types of model
that will be considered here have “hard constraints”, such that there is
only one choice for the remaining parameters <span class="math">\(\boldsymbol\theta\)</span>
that is compatible with the generated network, such that
Eq. <a class="reference internal" href="inference.html#equation-model-posterior-sum">(2)</a> simplifies to</p>
<div class="math" id="equation-model-posterior">
<span class="eqno">(4)</span>\[P(\boldsymbol b | \boldsymbol G) = \frac{P(\boldsymbol G|\boldsymbol\theta, \boldsymbol b)P(\boldsymbol\theta, \boldsymbol b)}{P(\boldsymbol G)}\]</div>
<p>with <span class="math">\(\boldsymbol\theta\)</span> above being the only choice compatible with
<span class="math">\(\boldsymbol G\)</span> and <span class="math">\(\boldsymbol b\)</span>. The inference procedures considered
here will consist in either finding a network partition that maximizes
Eq. <a class="reference internal" href="inference.html#equation-model-posterior">(4)</a>, or sampling different partitions according
its posterior probability.</p>
<p>As we will show below, this approach also enables the comparison of
<cite>different</cite> models according to statistical evidence (a.k.a. <cite>model
selection</cite>).</p>
<div class="section" id="minimum-description-length-mdl">
<h3>Minimum description length (MDL)<a class="headerlink" href="inference.html#minimum-description-length-mdl" title="Permalink to this headline">¶</a></h3>
<p>We note that Eq. <a class="reference internal" href="inference.html#equation-model-posterior">(4)</a> can be written as</p>
<div class="math">
\[P(\boldsymbol b | \boldsymbol G) = \frac{\exp(-\Sigma)}{P(\boldsymbol G)}\]</div>
<p>where</p>
<div class="math" id="equation-model-dl">
<span class="eqno">(5)</span>\[\Sigma = -\ln P(\boldsymbol G|\boldsymbol\theta, \boldsymbol b) - \ln P(\boldsymbol\theta, \boldsymbol b)\]</div>
<p>is called the <strong>description length</strong> of the network <span class="math">\(\boldsymbol
G\)</span>. It measures the amount of <a class="reference external" href="https://en.wikipedia.org/wiki/Information_theory">information</a> required to
describe the data, if we <a class="reference external" href="https://en.wikipedia.org/wiki/Entropy_encoding">encode</a> it using the
particular parametrization of the generative model given by
<span class="math">\(\boldsymbol\theta\)</span> and <span class="math">\(\boldsymbol b\)</span>, as well as the
parameters themselves. Therefore, if we choose to maximize the posterior
distribution of Eq. <a class="reference internal" href="inference.html#equation-model-posterior">(4)</a> it will be fully equivalent to
the so-called <a class="reference external" href="https://en.wikipedia.org/wiki/Minimum_description_length">minimum description length</a>
method. This approach corresponds to an implementation of <a class="reference external" href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam’s razor</a>, where the <cite>simplest</cite>
model is selected, among all possibilities with the same explanatory
power. The selection is based on the statistical evidence available, and
therefore will not <a class="reference external" href="https://en.wikipedia.org/wiki/Overfitting">overfit</a>, i.e. mistake stochastic
fluctuations for actual structure. In particular this means that we will
not find modules in networks if they could have arisen simply because of
stochastic fluctuations, as they do in fully random graphs
<a class="reference internal" href="inference.html#guimera-modularity-2004" id="id2">[guimera-modularity-2004]</a>.</p>
</div>
</div>
<div class="section" id="the-stochastic-block-model-sbm">
<h2>The stochastic block model (SBM)<a class="headerlink" href="inference.html#the-stochastic-block-model-sbm" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_block_model">stochastic block model</a> is arguably
the simplest generative process based on the notion of groups of
nodes <a class="reference internal" href="inference.html#holland-stochastic-1983" id="id3">[holland-stochastic-1983]</a>. The <a class="reference external" href="https://en.wikipedia.org/wiki/Microcanonical_ensemble">microcanonical</a> formulation
<a class="reference internal" href="inference.html#peixoto-nonparametric-2017" id="id4">[peixoto-nonparametric-2017]</a> of the basic or “traditional” version takes
as parameters the partition of the nodes into groups
<span class="math">\(\boldsymbol b\)</span> and a <span class="math">\(B\times B\)</span> matrix of edge counts
<span class="math">\(\boldsymbol e\)</span>, where <span class="math">\(e_{rs}\)</span> is the number of edges
between groups <span class="math">\(r\)</span> and <span class="math">\(s\)</span>. Given these constraints, the
edges are then placed randomly. Hence, nodes that belong to the same
group possess the same probability of being connected with other
nodes of the network.</p>
<p>An example of a possible parametrization is given in the following
figure.</p>
<table border="1" class="figure docutils">
<colgroup>
<col width="53%" />
<col width="47%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><div class="first last figure align-center" id="id39">
<a class="reference internal image-reference" href="../../_images/sbm-example-ers.svg"><img alt="../../_images/sbm-example-ers.svg" src="../../_images/sbm-example-ers.svg" width="300px" /></a>
<p class="caption"><span class="caption-text">Matrix of edge counts
<span class="math">\(\boldsymbol e\)</span> between
groups.</span></p>
</div>
</td>
<td><div class="first last figure align-center" id="id40">
<a class="reference internal image-reference" href="../../_images/sbm-example.svg"><img alt="../../_images/sbm-example.svg" src="../../_images/sbm-example.svg" width="300px" /></a>
<p class="caption"><span class="caption-text">Generated network.</span></p>
</div>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">We emphasize that no constraints are imposed on what <cite>kind</cite> of
modular structure is allowed, as the matrix of edge counts <span class="math">\(e\)</span>
is unconstrained. Hence, we can detect the putatively typical pattern
of <a class="reference external" href="https://en.wikipedia.org/wiki/Community_structure">“community structure”</a>, i.e. when
nodes are connected mostly to other nodes of the same group, if it
happens to be the most likely network description, but we can also
detect a large multiplicity of other patterns, such as <a class="reference external" href="https://en.wikipedia.org/wiki/Bipartite_graph">bipartiteness</a>, core-periphery,
and many others, all under the same inference framework.</p>
</div>
<p>Although quite general, the traditional model assumes that the edges are
placed randomly inside each group, and because of this the nodes that
belong to the same group tend to have very similar degrees. As it turns
out, this is often a poor model for many networks, which possess highly
heterogeneous degree distributions. A better model for such networks is
called the <cite>degree-corrected</cite> stochastic block model
<a class="reference internal" href="inference.html#karrer-stochastic-2011" id="id5">[karrer-stochastic-2011]</a>, and it is defined just like the traditional
model, with the addition of the degree sequence <span class="math">\(\boldsymbol k =
\{k_i\}\)</span> of the graph as an additional set of parameters (assuming again
a microcanonical formulation <a class="reference internal" href="inference.html#peixoto-nonparametric-2017" id="id6">[peixoto-nonparametric-2017]</a>).</p>
<div class="section" id="the-nested-stochastic-block-model">
<h3>The nested stochastic block model<a class="headerlink" href="inference.html#the-nested-stochastic-block-model" title="Permalink to this headline">¶</a></h3>
<p>The regular SBM has a drawback when applied to large networks. Namely,
it cannot be used to find relatively small groups, as the maximum number
of groups that can be found scales as
<span class="math">\(B_{\text{max}}=O(\sqrt{N})\)</span>, where <span class="math">\(N\)</span> is the number of
nodes in the network, if Bayesian inference is performed
<a class="reference internal" href="inference.html#peixoto-parsimonious-2013" id="id7">[peixoto-parsimonious-2013]</a>. In order to circumvent this, we need to
replace the noninformative priors used by a hierarchy of priors and
hyperpriors, which amounts to a <cite>nested SBM</cite>, where the groups
themselves are clustered into groups, and the matrix <span class="math">\(e\)</span> of edge
counts are generated from another SBM, and so on recursively
<a class="reference internal" href="inference.html#peixoto-hierarchical-2014" id="id8">[peixoto-hierarchical-2014]</a>, as illustrated below.</p>
<div class="figure align-center" id="id41">
<a class="reference internal image-reference" href="../../_images/nested-diagram.svg"><img alt="../../_images/nested-diagram.svg" src="../../_images/nested-diagram.svg" width="400px" /></a>
<p class="caption"><span class="caption-text">Example of a nested SBM with three levels.</span></p>
</div>
<p>With this model, the maximum number of groups that can be inferred
scales as <span class="math">\(B_{\text{max}}=O(N/\log(N))\)</span>. In addition to being able
to find small groups in large networks, this model also provides a
multilevel hierarchical description of the network. With such a
description, we can uncover structural patterns at multiple scales,
representing different levels of coarse-graining.</p>
</div>
</div>
<div class="section" id="inferring-the-best-partition">
<h2>Inferring the best partition<a class="headerlink" href="inference.html#inferring-the-best-partition" title="Permalink to this headline">¶</a></h2>
<p>The simplest and most efficient approach is to find the best
partition of the network by maximizing Eq. <a class="reference internal" href="inference.html#equation-model-posterior">(4)</a>
according to some version of the model. This is obtained via the
functions <a class="reference internal" href="../../inference.html#graph_tool.inference.minimize.minimize_blockmodel_dl" title="graph_tool.inference.minimize.minimize_blockmodel_dl"><code class="xref py py-func docutils literal notranslate"><span class="pre">minimize_blockmodel_dl()</span></code></a> or
<a class="reference internal" href="../../inference.html#graph_tool.inference.minimize.minimize_nested_blockmodel_dl" title="graph_tool.inference.minimize.minimize_nested_blockmodel_dl"><code class="xref py py-func docutils literal notranslate"><span class="pre">minimize_nested_blockmodel_dl()</span></code></a>, which
employs an agglomerative multilevel <a class="reference external" href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov chain Monte Carlo (MCMC)</a> algorithm
<a class="reference internal" href="inference.html#peixoto-efficient-2014" id="id9">[peixoto-efficient-2014]</a>.</p>
<p>We focus first on the non-nested model, and we illustrate its use with a
network of American football teams, which we load from the
<a class="reference internal" href="../../collection.html#module-graph_tool.collection" title="graph_tool.collection"><code class="xref py py-mod docutils literal notranslate"><span class="pre">collection</span></code></a> module:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;football&quot;</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
</pre></div>
</div>
<p>which yields</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;Graph object, undirected, with 115 vertices and 613 edges at 0x...&gt;
</pre></div>
</div>
<p>we then fit the degree-corrected model by calling</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">state</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">minimize_blockmodel_dl</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
</pre></div>
</div>
<p>This returns a <a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState" title="graph_tool.inference.blockmodel.BlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlockState</span></code></a> object that
includes the inference results.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The inference algorithm used is stochastic by nature, and may return
a different answer each time it is run. This may be due to the fact
that there are alternative partitions with similar probabilities, or
that the optimum is difficult to find. Note that the inference
problem here is, in general, <a class="reference external" href="https://en.wikipedia.org/wiki/NP-hardness">NP-Hard</a>, hence there is no
efficient algorithm that is guaranteed to always find the best
answer.</p>
<p class="last">Because of this, typically one would call the algorithm many times,
and select the partition with the largest posterior probability of
Eq. <a class="reference internal" href="inference.html#equation-model-posterior">(4)</a>, or equivalently, the minimum description
length of Eq. <a class="reference internal" href="inference.html#equation-model-dl">(5)</a>. The description length of a fit can be
obtained with the <a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState.entropy" title="graph_tool.inference.blockmodel.BlockState.entropy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">entropy()</span></code></a>
method. See also Sec. <a class="reference internal" href="inference.html#sec-model-selection"><span class="std std-ref">Hierarchical partitions</span></a> below.</p>
</div>
<p>We may perform a drawing of the partition obtained via the
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState.draw" title="graph_tool.inference.blockmodel.BlockState.draw"><code class="xref py py-mod docutils literal notranslate"><span class="pre">draw</span></code></a> method, that functions as a
convenience wrapper to the <a class="reference internal" href="../../draw.html#graph_tool.draw.graph_draw" title="graph_tool.draw.graph_draw"><code class="xref py py-func docutils literal notranslate"><span class="pre">graph_draw()</span></code></a> function</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">state</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">pos</span><span class="o">=</span><span class="n">g</span><span class="o">.</span><span class="n">vp</span><span class="o">.</span><span class="n">pos</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="s2">&quot;football-sbm-fit.svg&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>which yields the following image.</p>
<div class="figure align-center" id="id42">
<a class="reference internal image-reference" href="../../_images/football-sbm-fit.svg"><img alt="../../_images/football-sbm-fit.svg" src="../../_images/football-sbm-fit.svg" width="400px" /></a>
<p class="caption"><span class="caption-text">Stochastic block model inference of a network of American college
football teams. The colors correspond to inferred group membership of
the nodes.</span></p>
</div>
<p>We can obtain the group memberships as a
<a class="reference internal" href="../../graph_tool.html#graph_tool.PropertyMap" title="graph_tool.PropertyMap"><code class="xref py py-class docutils literal notranslate"><span class="pre">PropertyMap</span></code></a> on the vertices via the
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState.get_blocks" title="graph_tool.inference.blockmodel.BlockState.get_blocks"><code class="xref py py-mod docutils literal notranslate"><span class="pre">get_blocks</span></code></a> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get_blocks</span><span class="p">()</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="mi">10</span><span class="p">]</span>   <span class="c1"># group membership of vertex 10</span>
<span class="k">print</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
</pre></div>
</div>
<p>which yields:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>3
</pre></div>
</div>
<p>We may also access the matrix of edge counts between groups via
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState.get_matrix" title="graph_tool.inference.blockmodel.BlockState.get_matrix"><code class="xref py py-mod docutils literal notranslate"><span class="pre">get_matrix</span></code></a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">e</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get_matrix</span><span class="p">()</span>

<span class="n">matshow</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">todense</span><span class="p">())</span>
<span class="n">savefig</span><span class="p">(</span><span class="s2">&quot;football-edge-counts.svg&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center" id="id43">
<img alt="../../_images/football-edge-counts.svg" src="../../_images/football-edge-counts.svg" /><p class="caption"><span class="caption-text">Matrix of edge counts between groups.</span></p>
</div>
<p>We may obtain the same matrix of edge counts as a graph, which has
internal edge and vertex property maps with the edge and vertex counts,
respectively:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bg</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get_bg</span><span class="p">()</span>
<span class="n">ers</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">mrs</span>    <span class="c1"># edge counts</span>
<span class="n">nr</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">wr</span>      <span class="c1"># node counts</span>
</pre></div>
</div>
<div class="section" id="hierarchical-partitions">
<span id="sec-model-selection"></span><h3>Hierarchical partitions<a class="headerlink" href="inference.html#hierarchical-partitions" title="Permalink to this headline">¶</a></h3>
<p>The inference of the nested family of SBMs is done in a similar manner,
but we must use instead the
<a class="reference internal" href="../../inference.html#graph_tool.inference.minimize.minimize_nested_blockmodel_dl" title="graph_tool.inference.minimize.minimize_nested_blockmodel_dl"><code class="xref py py-func docutils literal notranslate"><span class="pre">minimize_nested_blockmodel_dl()</span></code></a> function. We
illustrate its use with the neural network of the <a class="reference external" href="https://en.wikipedia.org/wiki/Caenorhabditis_elegans">C. elegans</a> worm:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;celegansneural&quot;</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
</pre></div>
</div>
<p>which has 297 vertices and 2359 edges.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;Graph object, directed, with 297 vertices and 2359 edges at 0x...&gt;
</pre></div>
</div>
<p>A hierarchical fit of the degree-corrected model is performed as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">state</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">minimize_nested_blockmodel_dl</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
</pre></div>
</div>
<p>The object returned is an instance of a
<a class="reference internal" href="../../inference.html#graph_tool.inference.nested_blockmodel.NestedBlockState" title="graph_tool.inference.nested_blockmodel.NestedBlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">NestedBlockState</span></code></a> class, which
encapsulates the results. We can again draw the resulting hierarchical
clustering using the
<a class="reference internal" href="../../inference.html#graph_tool.inference.nested_blockmodel.NestedBlockState.draw" title="graph_tool.inference.nested_blockmodel.NestedBlockState.draw"><code class="xref py py-meth docutils literal notranslate"><span class="pre">draw()</span></code></a> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">state</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">output</span><span class="o">=</span><span class="s2">&quot;celegans-hsbm-fit.svg&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center" id="id44">
<img alt="../../_images/celegans-hsbm-fit.svg" src="../../_images/celegans-hsbm-fit.svg" /><p class="caption"><span class="caption-text">Most likely hierarchical partition of the neural network of
the <em>C. elegans</em> worm according to the nested degree-corrected SBM.</span></p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If the <code class="docutils literal notranslate"><span class="pre">output</span></code> parameter to
<a class="reference internal" href="../../inference.html#graph_tool.inference.nested_blockmodel.NestedBlockState.draw" title="graph_tool.inference.nested_blockmodel.NestedBlockState.draw"><code class="xref py py-meth docutils literal notranslate"><span class="pre">draw()</span></code></a> is omitted, an
interactive visualization is performed, where the user can re-order
the hierarchy nodes using the mouse and pressing the <code class="docutils literal notranslate"><span class="pre">r</span></code> key.</p>
</div>
<p>A summary of the inferred hierarchy can be obtained with the
<a class="reference internal" href="../../inference.html#graph_tool.inference.nested_blockmodel.NestedBlockState.print_summary" title="graph_tool.inference.nested_blockmodel.NestedBlockState.print_summary"><code class="xref py py-meth docutils literal notranslate"><span class="pre">print_summary()</span></code></a> method,
which shows the number of nodes and groups in all levels:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">state</span><span class="o">.</span><span class="n">print_summary</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>l: 0, N: 297, B: 17
l: 1, N: 17, B: 9
l: 2, N: 9, B: 3
l: 3, N: 3, B: 1
</pre></div>
</div>
<p>The hierarchical levels themselves are represented by individual
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState" title="graph_tool.inference.blockmodel.BlockState"><code class="xref py py-meth docutils literal notranslate"><span class="pre">BlockState()</span></code></a> instances obtained via the
<a class="reference internal" href="../../inference.html#graph_tool.inference.nested_blockmodel.NestedBlockState.get_levels" title="graph_tool.inference.nested_blockmodel.NestedBlockState.get_levels"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_levels()</span></code></a> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">levels</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get_levels</span><span class="p">()</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">levels</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;BlockState object with 17 blocks (17 nonempty), degree-corrected, for graph &lt;Graph object, directed, with 297 vertices and 2359 edges at 0x...&gt;, at 0x...&gt;
&lt;BlockState object with 9 blocks (9 nonempty), for graph &lt;Graph object, directed, with 17 vertices and 156 edges at 0x...&gt;, at 0x...&gt;
&lt;BlockState object with 3 blocks (3 nonempty), for graph &lt;Graph object, directed, with 9 vertices and 57 edges at 0x...&gt;, at 0x...&gt;
&lt;BlockState object with 1 blocks (1 nonempty), for graph &lt;Graph object, directed, with 3 vertices and 9 edges at 0x...&gt;, at 0x...&gt;
</pre></div>
</div>
<p>This means that we can inspect the hierarchical partition just as before:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">r</span> <span class="o">=</span> <span class="n">levels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_blocks</span><span class="p">()[</span><span class="mi">46</span><span class="p">]</span>    <span class="c1"># group membership of node 46 in level 0</span>
<span class="k">print</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">levels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_blocks</span><span class="p">()[</span><span class="n">r</span><span class="p">]</span>     <span class="c1"># group membership of node 46 in level 1</span>
<span class="k">print</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">levels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_blocks</span><span class="p">()[</span><span class="n">r</span><span class="p">]</span>     <span class="c1"># group membership of node 46 in level 2</span>
<span class="k">print</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>7
0
0
</pre></div>
</div>
</div>
<div class="section" id="model-selection">
<span id="id10"></span><h3>Model selection<a class="headerlink" href="inference.html#model-selection" title="Permalink to this headline">¶</a></h3>
<p>As mentioned above, one can select the best model according to the
choice that yields the smallest description length
<a class="reference internal" href="inference.html#peixoto-model-2016" id="id11">[peixoto-model-2016]</a>. For instance, in case of the <cite>C. elegans</cite> network
we have</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;celegansneural&quot;</span><span class="p">]</span>

<span class="n">state_ndc</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">minimize_nested_blockmodel_dl</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">deg_corr</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">state_dc</span>  <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">minimize_nested_blockmodel_dl</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">deg_corr</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Non-degree-corrected DL:</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">state_ndc</span><span class="o">.</span><span class="n">entropy</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Degree-corrected DL:</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">state_dc</span><span class="o">.</span><span class="n">entropy</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Non-degree-corrected DL:     8456.994339...
Degree-corrected DL:         8233.850036...
</pre></div>
</div>
<p>Since it yields the smallest description length, the degree-corrected
fit should be preferred. The statistical significance of the choice can
be accessed by inspecting the posterior odds ratio
<a class="reference internal" href="inference.html#peixoto-nonparametric-2017" id="id12">[peixoto-nonparametric-2017]</a></p>
<div class="math">
\[\begin{split}\Lambda &amp;= \frac{P(\boldsymbol b, \mathcal{H}_\text{NDC} | \boldsymbol G)}{P(\boldsymbol b, \mathcal{H}_\text{DC} | \boldsymbol G)} \\
        &amp;= \frac{P(\boldsymbol G, \boldsymbol b | \mathcal{H}_\text{NDC})}{P(\boldsymbol G, \boldsymbol b | \mathcal{H}_\text{DC})}\times\frac{P(\mathcal{H}_\text{NDC})}{P(\mathcal{H}_\text{DC})} \\
        &amp;= \exp(-\Delta\Sigma)\end{split}\]</div>
<p>where <span class="math">\(\mathcal{H}_\text{NDC}\)</span> and <span class="math">\(\mathcal{H}_\text{DC}\)</span>
correspond to the non-degree-corrected and degree-corrected model
hypotheses (assumed to be equally likely <cite>a priori</cite>), respectively, and
<span class="math">\(\Delta\Sigma\)</span> is the difference of the description length of both
fits. In our particular case, we have</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="sa">u</span><span class="s2">&quot;ln </span><span class="se">\u039b</span><span class="s2">: &quot;</span><span class="p">,</span> <span class="n">state_dc</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span> <span class="o">-</span> <span class="n">state_ndc</span><span class="o">.</span><span class="n">entropy</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>ln Λ:  -223.144303...
</pre></div>
</div>
<p>The precise threshold that should be used to decide when to <a class="reference external" href="https://en.wikipedia.org/wiki/Hypothesis_testing">reject a
hypothesis</a> is
subjective and context-dependent, but the value above implies that the
particular degree-corrected fit is around <span class="math">\(\mathrm{e}^{233} \approx 10^{96}\)</span>
times more likely than the non-degree corrected one, and hence it can be
safely concluded that it provides a substantially better fit.</p>
<p>Although it is often true that the degree-corrected model provides a
better fit for many empirical networks, there are also exceptions. For
example, for the American football network above, we have:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;football&quot;</span><span class="p">]</span>

<span class="n">state_ndc</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">minimize_nested_blockmodel_dl</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">deg_corr</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">state_dc</span>  <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">minimize_nested_blockmodel_dl</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">deg_corr</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Non-degree-corrected DL:</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">state_ndc</span><span class="o">.</span><span class="n">entropy</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Degree-corrected DL:</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">state_dc</span><span class="o">.</span><span class="n">entropy</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="sa">u</span><span class="s2">&quot;ln </span><span class="se">\u039b</span><span class="s2">:</span><span class="se">\t\t\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">state_ndc</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span> <span class="o">-</span> <span class="n">state_dc</span><span class="o">.</span><span class="n">entropy</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Non-degree-corrected DL:     1734.814739...
Degree-corrected DL:         1780.576716...
ln Λ:                         -45.761977...
</pre></div>
</div>
<p>Hence, with a posterior odds ratio of <span class="math">\(\Lambda \approx \mathrm{e}^{-45} \approx
10^{-19}\)</span> in favor of the non-degree-corrected model, it seems like the
degree-corrected variant is an unnecessarily complex description for
this network.</p>
</div>
</div>
<div class="section" id="sampling-from-the-posterior-distribution">
<span id="sampling"></span><h2>Sampling from the posterior distribution<a class="headerlink" href="inference.html#sampling-from-the-posterior-distribution" title="Permalink to this headline">¶</a></h2>
<p>When analyzing empirical networks, one should be open to the possibility
that there will be more than one fit of the SBM with similar posterior
probabilities. In such situations, one should instead <cite>sample</cite>
partitions from the posterior distribution, instead of simply finding
its maximum. One can then compute quantities that are averaged over the
different model fits, weighted according to their posterior
probabilities.</p>
<p>Full support for model averaging is implemented in <code class="docutils literal notranslate"><span class="pre">graph-tool</span></code> via an
efficient <a class="reference external" href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov chain Monte Carlo (MCMC)</a> algorithm
<a class="reference internal" href="inference.html#peixoto-efficient-2014" id="id14">[peixoto-efficient-2014]</a>. It works by attempting to move nodes into
different groups with specific probabilities, and <a class="reference external" href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">accepting or
rejecting</a>
such moves so that, after a sufficiently long time, the partitions will
be observed with the desired posterior probability. The algorithm is
designed so that its run-time (i.e. each sweep of the MCMC) is linear on
the number of edges in the network, and independent on the number of
groups being used in the model, and hence is suitable for use on very
large networks.</p>
<p>In order to perform such moves, one needs again to operate with
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState" title="graph_tool.inference.blockmodel.BlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlockState</span></code></a> or
<a class="reference internal" href="../../inference.html#graph_tool.inference.nested_blockmodel.NestedBlockState" title="graph_tool.inference.nested_blockmodel.NestedBlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">NestedBlockState</span></code></a> instances, and calling
their <a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState.mcmc_sweep" title="graph_tool.inference.blockmodel.BlockState.mcmc_sweep"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mcmc_sweep()</span></code></a> methods. For
example, the following will perform 1000 sweeps of the algorithm with
the network of characters in the novel Les Misérables, starting from a
random partition into 20 groups</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;lesmis&quot;</span><span class="p">]</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">BlockState</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>   <span class="c1"># This automatically initializes the state</span>
                                 <span class="c1"># with a random partition into B=20</span>
                                 <span class="c1"># nonempty groups; The user could</span>
                                 <span class="c1"># also pass an arbitrary initial</span>
                                 <span class="c1"># partition using the &#39;b&#39; parameter.</span>

<span class="c1"># Now we run 1,000 sweeps of the MCMC. Note that the number of groups</span>
<span class="c1"># is allowed to change, so it will eventually move from the initial</span>
<span class="c1"># value of B=20 to whatever is most appropriate for the data.</span>

<span class="n">dS</span><span class="p">,</span> <span class="n">nattempts</span><span class="p">,</span> <span class="n">nmoves</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">mcmc_sweep</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Change in description length:&quot;</span><span class="p">,</span> <span class="n">dS</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Number of accepted vertex moves:&quot;</span><span class="p">,</span> <span class="n">nmoves</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Change in description length: -365.317522...
Number of accepted vertex moves: 38213
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Starting from a random partition is rarely the best option, since it
may take a long time for it to equilibrate. It was done above simply
as an illustration on how to initialize
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState" title="graph_tool.inference.blockmodel.BlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlockState</span></code></a> by hand. Instead, a much
better option in practice is to start from an approximation to the
“ground state” obtained with
<a class="reference internal" href="../../inference.html#graph_tool.inference.minimize.minimize_blockmodel_dl" title="graph_tool.inference.minimize.minimize_blockmodel_dl"><code class="xref py py-func docutils literal notranslate"><span class="pre">minimize_blockmodel_dl()</span></code></a>, e.g.</p>
<blockquote class="last">
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">state</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">minimize_blockmodel_dl</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">B</span><span class="o">=</span><span class="n">g</span><span class="o">.</span><span class="n">num_vertices</span><span class="p">())</span>
<span class="n">dS</span><span class="p">,</span> <span class="n">nattempts</span><span class="p">,</span> <span class="n">nmoves</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">mcmc_sweep</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Change in description length:&quot;</span><span class="p">,</span> <span class="n">dS</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Number of accepted vertex moves:&quot;</span><span class="p">,</span> <span class="n">nmoves</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Change in description length: 1.660677...
Number of accepted vertex moves: 40461
</pre></div>
</div>
</div></blockquote>
</div>
<p>Although the above is sufficient to implement model averaging, there is a
convenience function called
<a class="reference internal" href="../../inference.html#graph_tool.inference.mcmc.mcmc_equilibrate" title="graph_tool.inference.mcmc.mcmc_equilibrate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mcmc_equilibrate()</span></code></a> that is intend to
simplify the detection of equilibration, by keeping track of the maximum
and minimum values of description length encountered and how many sweeps
have been made without a “record breaking” event. For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># We will accept equilibration if 10 sweeps are completed without a</span>
<span class="c1"># record breaking event, 2 consecutive times.</span>

<span class="n">gt</span><span class="o">.</span><span class="n">mcmc_equilibrate</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">wait</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">nbreaks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">mcmc_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<p>will output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>niter:     1  count:    0  breaks:  0  min_S: 706.26857  max_S: 708.14483  S: 708.14483  ΔS:      1.87626  moves:   418
niter:     2  count:    0  breaks:  0  min_S: 699.23453  max_S: 708.14483  S: 699.23453  ΔS:     -8.91030  moves:   409
niter:     3  count:    0  breaks:  0  min_S: 699.23453  max_S: 715.33531  S: 715.33531  ΔS:      16.1008  moves:   414
niter:     4  count:    0  breaks:  0  min_S: 699.23453  max_S: 723.13301  S: 723.13301  ΔS:      7.79770  moves:   391
niter:     5  count:    1  breaks:  0  min_S: 699.23453  max_S: 723.13301  S: 702.93354  ΔS:     -20.1995  moves:   411
niter:     6  count:    2  breaks:  0  min_S: 699.23453  max_S: 723.13301  S: 706.39029  ΔS:      3.45675  moves:   389
niter:     7  count:    3  breaks:  0  min_S: 699.23453  max_S: 723.13301  S: 706.80859  ΔS:     0.418293  moves:   404
niter:     8  count:    4  breaks:  0  min_S: 699.23453  max_S: 723.13301  S: 707.61960  ΔS:     0.811010  moves:   417
niter:     9  count:    5  breaks:  0  min_S: 699.23453  max_S: 723.13301  S: 706.46577  ΔS:     -1.15383  moves:   392
niter:    10  count:    6  breaks:  0  min_S: 699.23453  max_S: 723.13301  S: 714.34671  ΔS:      7.88094  moves:   410
niter:    11  count:    7  breaks:  0  min_S: 699.23453  max_S: 723.13301  S: 706.43194  ΔS:     -7.91477  moves:   383
niter:    12  count:    8  breaks:  0  min_S: 699.23453  max_S: 723.13301  S: 705.19434  ΔS:     -1.23760  moves:   405
niter:    13  count:    9  breaks:  0  min_S: 699.23453  max_S: 723.13301  S: 702.21395  ΔS:     -2.98039  moves:   423
niter:    14  count:    0  breaks:  1  min_S: 715.54878  max_S: 715.54878  S: 715.54878  ΔS:      13.3348  moves:   400
niter:    15  count:    0  breaks:  1  min_S: 715.54878  max_S: 716.65842  S: 716.65842  ΔS:      1.10964  moves:   413
niter:    16  count:    0  breaks:  1  min_S: 701.19994  max_S: 716.65842  S: 701.19994  ΔS:     -15.4585  moves:   382
niter:    17  count:    1  breaks:  1  min_S: 701.19994  max_S: 716.65842  S: 715.56997  ΔS:      14.3700  moves:   394
niter:    18  count:    0  breaks:  1  min_S: 701.19994  max_S: 719.25577  S: 719.25577  ΔS:      3.68580  moves:   404
niter:    19  count:    0  breaks:  1  min_S: 701.19994  max_S: 723.78811  S: 723.78811  ΔS:      4.53233  moves:   413
niter:    20  count:    1  breaks:  1  min_S: 701.19994  max_S: 723.78811  S: 709.77340  ΔS:     -14.0147  moves:   387
niter:    21  count:    2  breaks:  1  min_S: 701.19994  max_S: 723.78811  S: 714.14891  ΔS:      4.37551  moves:   419
niter:    22  count:    3  breaks:  1  min_S: 701.19994  max_S: 723.78811  S: 722.05875  ΔS:      7.90984  moves:   399
niter:    23  count:    4  breaks:  1  min_S: 701.19994  max_S: 723.78811  S: 714.32503  ΔS:     -7.73371  moves:   422
niter:    24  count:    5  breaks:  1  min_S: 701.19994  max_S: 723.78811  S: 708.53927  ΔS:     -5.78576  moves:   392
niter:    25  count:    6  breaks:  1  min_S: 701.19994  max_S: 723.78811  S: 714.05889  ΔS:      5.51962  moves:   404
niter:    26  count:    7  breaks:  1  min_S: 701.19994  max_S: 723.78811  S: 713.93196  ΔS:    -0.126937  moves:   414
niter:    27  count:    8  breaks:  1  min_S: 701.19994  max_S: 723.78811  S: 709.49863  ΔS:     -4.43333  moves:   410
niter:    28  count:    9  breaks:  1  min_S: 701.19994  max_S: 723.78811  S: 707.42167  ΔS:     -2.07696  moves:   397
niter:    29  count:    0  breaks:  1  min_S: 699.89982  max_S: 723.78811  S: 699.89982  ΔS:     -7.52185  moves:   388
niter:    30  count:    0  breaks:  1  min_S: 698.57305  max_S: 723.78811  S: 698.57305  ΔS:     -1.32677  moves:   391
niter:    31  count:    1  breaks:  1  min_S: 698.57305  max_S: 723.78811  S: 706.02629  ΔS:      7.45324  moves:   412
niter:    32  count:    2  breaks:  1  min_S: 698.57305  max_S: 723.78811  S: 701.97778  ΔS:     -4.04852  moves:   421
niter:    33  count:    3  breaks:  1  min_S: 698.57305  max_S: 723.78811  S: 707.50134  ΔS:      5.52356  moves:   410
niter:    34  count:    4  breaks:  1  min_S: 698.57305  max_S: 723.78811  S: 708.56686  ΔS:      1.06552  moves:   424
niter:    35  count:    0  breaks:  1  min_S: 698.57305  max_S: 724.07361  S: 724.07361  ΔS:      15.5067  moves:   399
niter:    36  count:    1  breaks:  1  min_S: 698.57305  max_S: 724.07361  S: 723.51969  ΔS:    -0.553915  moves:   384
niter:    37  count:    2  breaks:  1  min_S: 698.57305  max_S: 724.07361  S: 702.36708  ΔS:     -21.1526  moves:   406
niter:    38  count:    3  breaks:  1  min_S: 698.57305  max_S: 724.07361  S: 707.60129  ΔS:      5.23420  moves:   405
niter:    39  count:    4  breaks:  1  min_S: 698.57305  max_S: 724.07361  S: 709.67542  ΔS:      2.07413  moves:   400
niter:    40  count:    5  breaks:  1  min_S: 698.57305  max_S: 724.07361  S: 714.52753  ΔS:      4.85212  moves:   398
niter:    41  count:    6  breaks:  1  min_S: 698.57305  max_S: 724.07361  S: 707.86563  ΔS:     -6.66190  moves:   409
niter:    42  count:    7  breaks:  1  min_S: 698.57305  max_S: 724.07361  S: 718.80926  ΔS:      10.9436  moves:   400
niter:    43  count:    8  breaks:  1  min_S: 698.57305  max_S: 724.07361  S: 716.37312  ΔS:     -2.43615  moves:   378
niter:    44  count:    9  breaks:  1  min_S: 698.57305  max_S: 724.07361  S: 713.76944  ΔS:     -2.60368  moves:   399
niter:    45  count:   10  breaks:  2  min_S: 698.57305  max_S: 724.07361  S: 715.29009  ΔS:      1.52066  moves:   421
</pre></div>
</div>
<p>Note that the value of <code class="docutils literal notranslate"><span class="pre">wait</span></code> above was made purposefully low so that
the output would not be overly long. The most appropriate value requires
experimentation, but a typically good value is <code class="docutils literal notranslate"><span class="pre">wait=1000</span></code>.</p>
<p>The function <a class="reference internal" href="../../inference.html#graph_tool.inference.mcmc.mcmc_equilibrate" title="graph_tool.inference.mcmc.mcmc_equilibrate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mcmc_equilibrate()</span></code></a> accepts a
<code class="docutils literal notranslate"><span class="pre">callback</span></code> argument that takes an optional function to be invoked
after each call to
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState.mcmc_sweep" title="graph_tool.inference.blockmodel.BlockState.mcmc_sweep"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mcmc_sweep()</span></code></a>. This function
should accept a single parameter which will contain the actual
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState" title="graph_tool.inference.blockmodel.BlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlockState</span></code></a> instance. We will use this in
the example below to collect the posterior vertex marginals (via
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState.collect_vertex_marginals" title="graph_tool.inference.blockmodel.BlockState.collect_vertex_marginals"><code class="xref py py-class docutils literal notranslate"><span class="pre">collect_vertex_marginals</span></code></a>),
i.e. the posterior probability that a node belongs to a given group:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># We will first equilibrate the Markov chain</span>
<span class="n">gt</span><span class="o">.</span><span class="n">mcmc_equilibrate</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">wait</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">mcmc_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

<span class="n">pv</span> <span class="o">=</span> <span class="bp">None</span>

<span class="k">def</span> <span class="nf">collect_marginals</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
   <span class="k">global</span> <span class="n">pv</span>
   <span class="n">pv</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">collect_vertex_marginals</span><span class="p">(</span><span class="n">pv</span><span class="p">)</span>

<span class="c1"># Now we collect the marginals for exactly 100,000 sweeps, at</span>
<span class="c1"># intervals of 10 sweeps:</span>
<span class="n">gt</span><span class="o">.</span><span class="n">mcmc_equilibrate</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">force_niter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">mcmc_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                    <span class="n">callback</span><span class="o">=</span><span class="n">collect_marginals</span><span class="p">)</span>

<span class="c1"># Now the node marginals are stored in property map pv. We can</span>
<span class="c1"># visualize them as pie charts on the nodes:</span>
<span class="n">state</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">pos</span><span class="o">=</span><span class="n">g</span><span class="o">.</span><span class="n">vp</span><span class="o">.</span><span class="n">pos</span><span class="p">,</span> <span class="n">vertex_shape</span><span class="o">=</span><span class="s2">&quot;pie&quot;</span><span class="p">,</span> <span class="n">vertex_pie_fractions</span><span class="o">=</span><span class="n">pv</span><span class="p">,</span>
           <span class="n">edge_gradient</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="s2">&quot;lesmis-sbm-marginals.svg&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center" id="id45">
<a class="reference internal image-reference" href="../../_images/lesmis-sbm-marginals.svg"><img alt="../../_images/lesmis-sbm-marginals.svg" src="../../_images/lesmis-sbm-marginals.svg" width="450px" /></a>
<p class="caption"><span class="caption-text">Marginal probabilities of group memberships of the network of
characters in the novel Les Misérables, according to the
degree-corrected SBM. The <a class="reference external" href="https://en.wikipedia.org/wiki/Pie_chart">pie fractions</a> on the nodes correspond
to the probability of being in group associated with the respective
color.</span></p>
</div>
<p>We can also obtain a marginal probability on the number of groups
itself, as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">num_vertices</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">collect_num_groups</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">get_nonempty_B</span><span class="p">()</span>
    <span class="n">h</span><span class="p">[</span><span class="n">B</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Now we collect the marginals for exactly 100,000 sweeps, at</span>
<span class="c1"># intervals of 10 sweeps:</span>
<span class="n">gt</span><span class="o">.</span><span class="n">mcmc_equilibrate</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">force_niter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">mcmc_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                    <span class="n">callback</span><span class="o">=</span><span class="n">collect_num_groups</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center" id="id46">
<img alt="../../_images/lesmis-B-posterior.svg" src="../../_images/lesmis-B-posterior.svg" /><p class="caption"><span class="caption-text">Marginal posterior probability of the number of nonempty groups for
the network of characters in the novel Les Misérables, according to
the degree-corrected SBM.</span></p>
</div>
<div class="section" id="id15">
<h3>Hierarchical partitions<a class="headerlink" href="inference.html#id15" title="Permalink to this headline">¶</a></h3>
<p>We can also perform model averaging using the nested SBM, which will
give us a distribution over hierarchies. The whole procedure is fairly
analogous, but now we make use of
<a class="reference internal" href="../../inference.html#graph_tool.inference.nested_blockmodel.NestedBlockState" title="graph_tool.inference.nested_blockmodel.NestedBlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">NestedBlockState</span></code></a> instances.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When using <a class="reference internal" href="../../inference.html#graph_tool.inference.nested_blockmodel.NestedBlockState" title="graph_tool.inference.nested_blockmodel.NestedBlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">NestedBlockState</span></code></a> instances
to perform model averaging, they need to be constructed with the
option <code class="docutils literal notranslate"><span class="pre">sampling=True</span></code>.</p>
</div>
<p>Here we perform the sampling of hierarchical partitions using the same
network as above.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;lesmis&quot;</span><span class="p">]</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">minimize_nested_blockmodel_dl</span><span class="p">(</span><span class="n">g</span><span class="p">)</span> <span class="c1"># Initialize he Markov</span>
                                            <span class="c1"># chain from the &quot;ground</span>
                                            <span class="c1"># state&quot;</span>

<span class="c1"># Before doing model averaging, the need to create a NestedBlockState</span>
<span class="c1"># by passing sampling = True.</span>

<span class="c1"># We also want to increase the maximum hierarchy depth to L = 10</span>

<span class="c1"># We can do both of the above by copying.</span>

<span class="n">bs</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get_bs</span><span class="p">()</span>                     <span class="c1"># Get hierarchical partition.</span>
<span class="n">bs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">10</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">bs</span><span class="p">))</span>    <span class="c1"># Augment it to L = 10 with</span>
                                        <span class="c1"># single-group levels.</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">sampling</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Now we run 1000 sweeps of the MCMC</span>

<span class="n">dS</span><span class="p">,</span> <span class="n">nattempts</span><span class="p">,</span> <span class="n">nmoves</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">mcmc_sweep</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Change in description length:&quot;</span><span class="p">,</span> <span class="n">dS</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Number of accepted vertex moves:&quot;</span><span class="p">,</span> <span class="n">nmoves</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Change in description length: 2.371018...
Number of accepted vertex moves: 56087
</pre></div>
</div>
<p>Similarly to the the non-nested case, we can use
<a class="reference internal" href="../../inference.html#graph_tool.inference.mcmc.mcmc_equilibrate" title="graph_tool.inference.mcmc.mcmc_equilibrate"><code class="xref py py-func docutils literal notranslate"><span class="pre">mcmc_equilibrate()</span></code></a> to do most of the boring
work, and we can now obtain vertex marginals on all hierarchical levels:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># We will first equilibrate the Markov chain</span>
<span class="n">gt</span><span class="o">.</span><span class="n">mcmc_equilibrate</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">wait</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">mcmc_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

<span class="n">pv</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">get_levels</span><span class="p">())</span>

<span class="k">def</span> <span class="nf">collect_marginals</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
   <span class="k">global</span> <span class="n">pv</span>
   <span class="n">pv</span> <span class="o">=</span> <span class="p">[</span><span class="n">sl</span><span class="o">.</span><span class="n">collect_vertex_marginals</span><span class="p">(</span><span class="n">pv</span><span class="p">[</span><span class="n">l</span><span class="p">])</span> <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">sl</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">get_levels</span><span class="p">())]</span>

<span class="c1"># Now we collect the marginals for exactly 100,000 sweeps</span>
<span class="n">gt</span><span class="o">.</span><span class="n">mcmc_equilibrate</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">force_niter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">mcmc_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                    <span class="n">callback</span><span class="o">=</span><span class="n">collect_marginals</span><span class="p">)</span>

<span class="c1"># Now the node marginals for all levels are stored in property map</span>
<span class="c1"># list pv. We can visualize the first level as pie charts on the nodes:</span>
<span class="n">state_0</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get_levels</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">state_0</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">pos</span><span class="o">=</span><span class="n">g</span><span class="o">.</span><span class="n">vp</span><span class="o">.</span><span class="n">pos</span><span class="p">,</span> <span class="n">vertex_shape</span><span class="o">=</span><span class="s2">&quot;pie&quot;</span><span class="p">,</span> <span class="n">vertex_pie_fractions</span><span class="o">=</span><span class="n">pv</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
             <span class="n">edge_gradient</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="s2">&quot;lesmis-nested-sbm-marginals.svg&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center" id="id47">
<a class="reference internal image-reference" href="../../_images/lesmis-nested-sbm-marginals.svg"><img alt="../../_images/lesmis-nested-sbm-marginals.svg" src="../../_images/lesmis-nested-sbm-marginals.svg" width="450px" /></a>
<p class="caption"><span class="caption-text">Marginal probabilities of group memberships of the network of
characters in the novel Les Misérables, according to the nested
degree-corrected SBM. The pie fractions on the nodes correspond to
the probability of being in group associated with the respective
color.</span></p>
</div>
<p>We can also obtain a marginal probability of the number of groups
itself, as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">num_vertices</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">state</span><span class="o">.</span><span class="n">get_levels</span><span class="p">()]</span>

<span class="k">def</span> <span class="nf">collect_num_groups</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">sl</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">get_levels</span><span class="p">()):</span>
       <span class="n">B</span> <span class="o">=</span> <span class="n">sl</span><span class="o">.</span><span class="n">get_nonempty_B</span><span class="p">()</span>
       <span class="n">h</span><span class="p">[</span><span class="n">l</span><span class="p">][</span><span class="n">B</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="c1"># Now we collect the marginal distribution for exactly 100,000 sweeps</span>
<span class="n">gt</span><span class="o">.</span><span class="n">mcmc_equilibrate</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">force_niter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">mcmc_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                    <span class="n">callback</span><span class="o">=</span><span class="n">collect_num_groups</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center" id="id48">
<img alt="../../_images/lesmis-nested-B-posterior.svg" src="../../_images/lesmis-nested-B-posterior.svg" /><p class="caption"><span class="caption-text">Marginal posterior probability of the number of nonempty groups
<span class="math">\(B_l\)</span> at each hierarchy level <span class="math">\(l\)</span> for the network of
characters in the novel Les Misérables, according to the nested
degree-corrected SBM.</span></p>
</div>
<p>Below we obtain some hierarchical partitions sampled from the posterior
distribution.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">state</span><span class="o">.</span><span class="n">mcmc_sweep</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">state</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">output</span><span class="o">=</span><span class="s2">&quot;lesmis-partition-sample-</span><span class="si">%i</span><span class="s2">.svg&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="n">empty_branches</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="../../_images/lesmis-partition-sample-0.svg"><img alt="../../_images/lesmis-partition-sample-0.svg" src="../../_images/lesmis-partition-sample-0.svg" width="200px" /></a>
<a class="reference internal image-reference" href="../../_images/lesmis-partition-sample-1.svg"><img alt="../../_images/lesmis-partition-sample-1.svg" src="../../_images/lesmis-partition-sample-1.svg" width="200px" /></a>
<a class="reference internal image-reference" href="../../_images/lesmis-partition-sample-2.svg"><img alt="../../_images/lesmis-partition-sample-2.svg" src="../../_images/lesmis-partition-sample-2.svg" width="200px" /></a>
<a class="reference internal image-reference" href="../../_images/lesmis-partition-sample-3.svg"><img alt="../../_images/lesmis-partition-sample-3.svg" src="../../_images/lesmis-partition-sample-3.svg" width="200px" /></a>
<a class="reference internal image-reference" href="../../_images/lesmis-partition-sample-4.svg"><img alt="../../_images/lesmis-partition-sample-4.svg" src="../../_images/lesmis-partition-sample-4.svg" width="200px" /></a>
<a class="reference internal image-reference" href="../../_images/lesmis-partition-sample-5.svg"><img alt="../../_images/lesmis-partition-sample-5.svg" src="../../_images/lesmis-partition-sample-5.svg" width="200px" /></a>
<a class="reference internal image-reference" href="../../_images/lesmis-partition-sample-6.svg"><img alt="../../_images/lesmis-partition-sample-6.svg" src="../../_images/lesmis-partition-sample-6.svg" width="200px" /></a>
<a class="reference internal image-reference" href="../../_images/lesmis-partition-sample-7.svg"><img alt="../../_images/lesmis-partition-sample-7.svg" src="../../_images/lesmis-partition-sample-7.svg" width="200px" /></a>
<a class="reference internal image-reference" href="../../_images/lesmis-partition-sample-8.svg"><img alt="../../_images/lesmis-partition-sample-8.svg" src="../../_images/lesmis-partition-sample-8.svg" width="200px" /></a>
<a class="reference internal image-reference" href="../../_images/lesmis-partition-sample-9.svg"><img alt="../../_images/lesmis-partition-sample-9.svg" src="../../_images/lesmis-partition-sample-9.svg" width="200px" /></a>
</div>
<div class="section" id="model-class-selection">
<h3>Model class selection<a class="headerlink" href="inference.html#model-class-selection" title="Permalink to this headline">¶</a></h3>
<p>When averaging over partitions, we may be interested in evaluating which
<strong>model class</strong> provides a better fit of the data, considering all
possible parameter choices. This is done by evaluating the model
evidence summed over all possible partitions <a class="reference internal" href="inference.html#peixoto-nonparametric-2017" id="id16">[peixoto-nonparametric-2017]</a>:</p>
<div class="math">
\[P(\boldsymbol G) = \sum_{\boldsymbol\theta,\boldsymbol b}P(\boldsymbol G,\boldsymbol\theta, \boldsymbol b) =  \sum_{\boldsymbol b}P(\boldsymbol G,\boldsymbol b).\]</div>
<p>This quantity is analogous to a <a class="reference external" href="https://en.wikipedia.org/wiki/Partition_function_(statistical_mechanics)">partition function</a>
in statistical physics, which we can write more conveniently as a
negative <a class="reference external" href="https://en.wikipedia.org/wiki/Thermodynamic_free_energy">free energy</a> by taking
its logarithm</p>
<div class="math" id="equation-free-energy">
<span class="eqno">(6)</span>\[\begin{split}\ln P(\boldsymbol G) = \underbrace{\sum_{\boldsymbol b}q(\boldsymbol b)\ln P(\boldsymbol G,\boldsymbol b)}_{-\left&lt;\Sigma\right&gt;}\;
           \underbrace{- \sum_{\boldsymbol b}q(\boldsymbol b)\ln q(\boldsymbol b)}_{\mathcal{S}}\end{split}\]</div>
<p>where</p>
<div class="math">
\[q(\boldsymbol b) = \frac{P(\boldsymbol G,\boldsymbol b)}{\sum_{\boldsymbol b'}P(\boldsymbol G,\boldsymbol b')}\]</div>
<p>is the posterior probability of partition <span class="math">\(\boldsymbol b\)</span>. The
first term of Eq. <a class="reference internal" href="inference.html#equation-free-energy">(6)</a> (the “negative energy”) is minus the
average of description length <span class="math">\(\left&lt;\Sigma\right&gt;\)</span>, weighted
according to the posterior distribution. The second term
<span class="math">\(\mathcal{S}\)</span> is the <a class="reference external" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">entropy</a> of the
posterior distribution, and measures, in a sense, the “quality of fit”
of the model: If the posterior is very “peaked”, i.e. dominated by a
single partition with a very large probability, the entropy will tend to
zero. However, if there are many partitions with similar probabilities
— meaning that there is no single partition that describes the network
uniquely well — it will take a large value instead.</p>
<p>Since the MCMC algorithm samples partitions from the distribution
<span class="math">\(q(\boldsymbol b)\)</span>, it can be used to compute
<span class="math">\(\left&lt;\Sigma\right&gt;\)</span> easily, simply by averaging the description
length values encountered by sampling from the posterior distribution
many times.</p>
<p>The computation of the posterior entropy <span class="math">\(\mathcal{S}\)</span>, however,
is significantly more difficult, since it involves measuring the precise
value of <span class="math">\(q(\boldsymbol b)\)</span>. A direct “brute force” computation of
<span class="math">\(\mathcal{S}\)</span> is implemented via
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState.collect_partition_histogram" title="graph_tool.inference.blockmodel.BlockState.collect_partition_histogram"><code class="xref py py-meth docutils literal notranslate"><span class="pre">collect_partition_histogram()</span></code></a> and
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.microstate_entropy" title="graph_tool.inference.blockmodel.microstate_entropy"><code class="xref py py-func docutils literal notranslate"><span class="pre">microstate_entropy()</span></code></a>, however this is only
feasible for very small networks. For larger networks, we are forced to
perform approximations. The simplest is a “mean field” one, where we
assume the posterior factorizes as</p>
<div class="math">
\[q(\boldsymbol b) \approx \prod_i{q_i(b_i)}\]</div>
<p>where</p>
<div class="math">
\[q_i(r) = P(b_i = r | \boldsymbol G)\]</div>
<p>is the marginal group membership distribution of node <span class="math">\(i\)</span>. This
yields an entropy value given by</p>
<div class="math">
\[S \approx -\sum_i\sum_rq_i(r)\ln q_i(r).\]</div>
<p>This approximation should be seen as an upper bound, since any existing
correlation between the nodes (which are ignored here) will yield
smaller entropy values.</p>
<p>A more accurate assumption is called the <cite>Bethe approximation</cite>
<a class="reference internal" href="inference.html#mezard-information-2009" id="id17">[mezard-information-2009]</a>, and takes into account the correlation
between adjacent nodes in the network,</p>
<div class="math">
\[\begin{split}q(\boldsymbol b) \approx \prod_{i&lt;j}q_{ij}(b_i,b_j)^{A_{ij}}\prod_iq_i(b_i)^{1-k_i}\end{split}\]</div>
<p>where <span class="math">\(A_{ij}\)</span> is the <a class="reference external" href="https://en.wikipedia.org/wiki/Adjacency_matrix">adjacency matrix</a>, <span class="math">\(k_i\)</span> is the
degree of node <span class="math">\(i\)</span>, and</p>
<div class="math">
\[q_{ij}(r, s) = P(b_i = r, b_j = s|\boldsymbol G)\]</div>
<p>is the joint group membership distribution of nodes <span class="math">\(i\)</span> and
<span class="math">\(j\)</span> (a.k.a. the <cite>edge marginals</cite>). This yields an entropy value
given by</p>
<div class="math">
\[\begin{split}S \approx -\sum_{i&lt;j}A_{ij}\sum_{rs}q_{ij}(r,s)\ln q_{ij}(r,s) - \sum_i(1-k_i)\sum_rq_i(r)\ln q_i(r).\end{split}\]</div>
<p>Typically, this approximation yields smaller values than the mean field
one, and is generally considered to be superior. However, formally, it
depends on the graph being sufficiently locally “tree-like”, and the
posterior being indeed strongly correlated with the adjacency matrix
itself — two characteristics which do not hold in general. Although
the approximation often gives reasonable results even when these
conditions do not strictly hold, in some situations when they are
strongly violated this approach can yield meaningless values, such as a
negative entropy. Therefore, it is useful to compare both approaches
whenever possible.</p>
<p>With these approximations, it possible to estimate the full model
evidence efficiently, as we show below, using
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState.collect_vertex_marginals" title="graph_tool.inference.blockmodel.BlockState.collect_vertex_marginals"><code class="xref py py-meth docutils literal notranslate"><span class="pre">collect_vertex_marginals()</span></code></a>,
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState.collect_edge_marginals" title="graph_tool.inference.blockmodel.BlockState.collect_edge_marginals"><code class="xref py py-meth docutils literal notranslate"><span class="pre">collect_edge_marginals()</span></code></a>,
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.mf_entropy" title="graph_tool.inference.blockmodel.mf_entropy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mf_entropy()</span></code></a> and
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.bethe_entropy" title="graph_tool.inference.blockmodel.bethe_entropy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">bethe_entropy()</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;lesmis&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">deg_corr</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">]:</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">minimize_blockmodel_dl</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">deg_corr</span><span class="o">=</span><span class="n">deg_corr</span><span class="p">)</span>     <span class="c1"># Initialize the Markov</span>
                                                                <span class="c1"># chain from the &quot;ground</span>
                                                                <span class="c1"># state&quot;</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">B</span><span class="o">=</span><span class="n">g</span><span class="o">.</span><span class="n">num_vertices</span><span class="p">())</span>

    <span class="n">dls</span> <span class="o">=</span> <span class="p">[]</span>         <span class="c1"># description length history</span>
    <span class="n">vm</span> <span class="o">=</span> <span class="bp">None</span>        <span class="c1"># vertex marginals</span>
    <span class="n">em</span> <span class="o">=</span> <span class="bp">None</span>        <span class="c1"># edge marginals</span>

    <span class="k">def</span> <span class="nf">collect_marginals</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
        <span class="k">global</span> <span class="n">vm</span><span class="p">,</span> <span class="n">em</span>
        <span class="n">vm</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">collect_vertex_marginals</span><span class="p">(</span><span class="n">vm</span><span class="p">)</span>
        <span class="n">em</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">collect_edge_marginals</span><span class="p">(</span><span class="n">em</span><span class="p">)</span>
        <span class="n">dls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">entropy</span><span class="p">())</span>

    <span class="c1"># Now we collect the marginal distributions for exactly 200,000 sweeps</span>
    <span class="n">gt</span><span class="o">.</span><span class="n">mcmc_equilibrate</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">force_niter</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">mcmc_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                        <span class="n">callback</span><span class="o">=</span><span class="n">collect_marginals</span><span class="p">)</span>

    <span class="n">S_mf</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">mf_entropy</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">vm</span><span class="p">)</span>
    <span class="n">S_bethe</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">bethe_entropy</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">em</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">L</span> <span class="o">=</span> <span class="o">-</span><span class="n">mean</span><span class="p">(</span><span class="n">dls</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Model evidence for deg_corr = </span><span class="si">%s</span><span class="s2">:&quot;</span> <span class="o">%</span> <span class="n">deg_corr</span><span class="p">,</span>
          <span class="n">L</span> <span class="o">+</span> <span class="n">S_mf</span><span class="p">,</span> <span class="s2">&quot;(mean field),&quot;</span><span class="p">,</span> <span class="n">L</span> <span class="o">+</span> <span class="n">S_bethe</span><span class="p">,</span> <span class="s2">&quot;(Bethe)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Model evidence for deg_corr = True: -569.590426... (mean field), -817.788531... (Bethe)
Model evidence for deg_corr = False: -587.028530... (mean field), -736.990655... (Bethe)
</pre></div>
</div>
<p>If we consider the more accurate approximation, the outcome shows a
preference for the non-degree-corrected model.</p>
<p>When using the nested model, the approach is entirely analogous. The
only difference now is that we have a hierarchical partition
<span class="math">\(\{\boldsymbol b_l\}\)</span> in the equations above, instead of simply
<span class="math">\(\boldsymbol b\)</span>. In order to make the approach tractable, we
assume the factorization</p>
<div class="math">
\[q(\{\boldsymbol b_l\}) \approx \prod_lq_l(\boldsymbol b_l)\]</div>
<p>where <span class="math">\(q_l(\boldsymbol b_l)\)</span> is the marginal posterior for the
partition at level <span class="math">\(l\)</span>. For <span class="math">\(q_0(\boldsymbol b_0)\)</span> we may
use again either the mean-field or Bethe approximations, however for
<span class="math">\(l&gt;0\)</span> only the mean-field approximation is applicable, since the
adjacency matrix of the higher layers is not constant. We show below the
approach for the same network, using the nested model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;lesmis&quot;</span><span class="p">]</span>

<span class="n">nL</span> <span class="o">=</span> <span class="mi">10</span>

<span class="k">for</span> <span class="n">deg_corr</span> <span class="ow">in</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">]:</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">minimize_nested_blockmodel_dl</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">deg_corr</span><span class="o">=</span><span class="n">deg_corr</span><span class="p">)</span>     <span class="c1"># Initialize the Markov</span>
                                                                       <span class="c1"># chain from the &quot;ground</span>
                                                                       <span class="c1"># state&quot;</span>
    <span class="n">bs</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get_bs</span><span class="p">()</span>                     <span class="c1"># Get hierarchical partition.</span>
    <span class="n">bs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span> <span class="o">*</span> <span class="p">(</span><span class="n">nL</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">bs</span><span class="p">))</span>    <span class="c1"># Augment it to L = 10 with</span>
                                            <span class="c1"># single-group levels.</span>

    <span class="n">state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">sampling</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">dls</span> <span class="o">=</span> <span class="p">[]</span>                               <span class="c1"># description length history</span>
    <span class="n">vm</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">get_levels</span><span class="p">())</span>  <span class="c1"># vertex marginals</span>
    <span class="n">em</span> <span class="o">=</span> <span class="bp">None</span>                              <span class="c1"># edge marginals</span>

    <span class="k">def</span> <span class="nf">collect_marginals</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
        <span class="k">global</span> <span class="n">vm</span><span class="p">,</span> <span class="n">em</span>
        <span class="n">levels</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">get_levels</span><span class="p">()</span>
        <span class="n">vm</span> <span class="o">=</span> <span class="p">[</span><span class="n">sl</span><span class="o">.</span><span class="n">collect_vertex_marginals</span><span class="p">(</span><span class="n">vm</span><span class="p">[</span><span class="n">l</span><span class="p">])</span> <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">sl</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">levels</span><span class="p">)]</span>
        <span class="n">em</span> <span class="o">=</span> <span class="n">levels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">collect_edge_marginals</span><span class="p">(</span><span class="n">em</span><span class="p">)</span>
        <span class="n">dls</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">entropy</span><span class="p">())</span>

    <span class="c1"># Now we collect the marginal distributions for exactly 200,000 sweeps</span>
    <span class="n">gt</span><span class="o">.</span><span class="n">mcmc_equilibrate</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">force_niter</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">mcmc_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                        <span class="n">callback</span><span class="o">=</span><span class="n">collect_marginals</span><span class="p">)</span>

    <span class="n">S_mf</span> <span class="o">=</span> <span class="p">[</span><span class="n">gt</span><span class="o">.</span><span class="n">mf_entropy</span><span class="p">(</span><span class="n">sl</span><span class="o">.</span><span class="n">g</span><span class="p">,</span> <span class="n">vm</span><span class="p">[</span><span class="n">l</span><span class="p">])</span> <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">sl</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">get_levels</span><span class="p">())]</span>
    <span class="n">S_bethe</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">bethe_entropy</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">em</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">L</span> <span class="o">=</span> <span class="o">-</span><span class="n">mean</span><span class="p">(</span><span class="n">dls</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Model evidence for deg_corr = </span><span class="si">%s</span><span class="s2">:&quot;</span> <span class="o">%</span> <span class="n">deg_corr</span><span class="p">,</span>
          <span class="n">L</span> <span class="o">+</span> <span class="nb">sum</span><span class="p">(</span><span class="n">S_mf</span><span class="p">),</span> <span class="s2">&quot;(mean field),&quot;</span><span class="p">,</span> <span class="n">L</span> <span class="o">+</span> <span class="n">S_bethe</span> <span class="o">+</span> <span class="nb">sum</span><span class="p">(</span><span class="n">S_mf</span><span class="p">[</span><span class="mi">1</span><span class="p">:]),</span> <span class="s2">&quot;(Bethe)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Model evidence for deg_corr = True: -551.228195... (mean field), -740.460493... (Bethe)
Model evidence for deg_corr = False: -544.660366... (mean field), -649.135026... (Bethe)
</pre></div>
</div>
<p>The results are similar: If we consider the most accurate approximation,
the non-degree-corrected model possesses the largest evidence. Note also
that we observe a better evidence for the nested models themselves, when
comparing to the evidences for the non-nested model — which is not
quite surprising, since the non-nested model is a special case of the
nested one.</p>
</div>
</div>
<div class="section" id="edge-weights-and-covariates">
<span id="weights"></span><h2>Edge weights and covariates<a class="headerlink" href="inference.html#edge-weights-and-covariates" title="Permalink to this headline">¶</a></h2>
<p>Very often networks cannot be completely represented by simple graphs,
but instead have arbitrary “weights” <span class="math">\(x_{ij}\)</span> on the edges. Edge
weights can be continuous or discrete numbers, and either strictly
positive or positive or negative, depending on context. The SBM can be
extended to cover these cases by treating edge weights as covariates
that are sampled from some distribution conditioned on the node
partition <a class="reference internal" href="inference.html#aicher-learning-2015" id="id18">[aicher-learning-2015]</a> <a class="reference internal" href="inference.html#peixoto-weighted-2017" id="id19">[peixoto-weighted-2017]</a>, i.e.</p>
<div class="math">
\[P(\boldsymbol x,\boldsymbol G|\boldsymbol b) =
P(\boldsymbol x|\boldsymbol G,\boldsymbol b) P(\boldsymbol G|\boldsymbol b),\]</div>
<p>where <span class="math">\(P(\boldsymbol G|\boldsymbol b)\)</span> is the likelihood of the
unweighted SBM described previously, and <span class="math">\(P(\boldsymbol
x|\boldsymbol G,\boldsymbol b)\)</span> is the integrated likelihood of the edge
weights</p>
<div class="math">
\[P(\boldsymbol x|\boldsymbol G,\boldsymbol b) =
\prod_{r\le s}\int P({\boldsymbol x}_{rs}|\gamma)P(\gamma)\,\mathrm{d}\gamma,\]</div>
<p>where <span class="math">\(P({\boldsymbol x}_{rs}|\gamma)\)</span> is some model for the weights
<span class="math">\({\boldsymbol x}_{rs}\)</span> between groups <span class="math">\((r,s)\)</span>, conditioned on
some parameter <span class="math">\(\gamma\)</span>, sampled from its prior
<span class="math">\(P(\gamma)\)</span>. A hierarchical version of the model can also be
implemented by replacing this prior by a nested sequence of priors and
hyperpriors, as described in <a class="reference internal" href="inference.html#peixoto-weighted-2017" id="id20">[peixoto-weighted-2017]</a>. The posterior
partition distribution is then simply</p>
<div class="math">
\[P(\boldsymbol b | \boldsymbol G,\boldsymbol x) =
\frac{P(\boldsymbol x|\boldsymbol G,\boldsymbol b) P(\boldsymbol G|\boldsymbol b)
      P(\boldsymbol b)}{P(\boldsymbol G,\boldsymbol x)},\]</div>
<p>which can be sampled from, or maximized, just like with the unweighted
case, but will use the information on the weights to guide the partitions.</p>
<p>A variety of weight models is supported, reflecting different kinds of
edge covariates:</p>
<table border="1" class="colwidths-given docutils align-center">
<colgroup>
<col width="40%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Name</th>
<th class="head">Domain</th>
<th class="head">Bounds</th>
<th class="head">Shape</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">&quot;real-exponential&quot;</span></code></td>
<td>Real    <span class="math">\((\mathbb{R})\)</span></td>
<td><span class="math">\([0,\infty]\)</span></td>
<td><a class="reference external" href="https://en.wikipedia.org/wiki/Exponential_distribution">Exponential</a></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">&quot;real-normal&quot;</span></code></td>
<td>Real    <span class="math">\((\mathbb{R})\)</span></td>
<td><span class="math">\([-\infty,\infty]\)</span></td>
<td><a class="reference external" href="https://en.wikipedia.org/wiki/Normal_distribution">Normal</a></td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">&quot;discrete-geometric&quot;</span></code></td>
<td>Natural <span class="math">\((\mathbb{N})\)</span></td>
<td><span class="math">\([0,\infty]\)</span></td>
<td><a class="reference external" href="https://en.wikipedia.org/wiki/Geometric_distribution">Geometric</a></td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">&quot;discrete-binomial&quot;</span></code></td>
<td>Natural <span class="math">\((\mathbb{N})\)</span></td>
<td><span class="math">\([0,M]\)</span></td>
<td><a class="reference external" href="https://en.wikipedia.org/wiki/Binomial_distribution">Binomial</a></td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">&quot;discrete-poisson&quot;</span></code></td>
<td>Natural <span class="math">\((\mathbb{N})\)</span></td>
<td><span class="math">\([0,\infty]\)</span></td>
<td><a class="reference external" href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson</a></td>
</tr>
</tbody>
</table>
<p>In fact, the actual model implements <a class="reference external" href="https://en.wikipedia.org/wiki/Microcanonical_ensemble">microcanonical</a> versions of
these distributions that are asymptotically equivalent, as described in
<a class="reference internal" href="inference.html#peixoto-weighted-2017" id="id22">[peixoto-weighted-2017]</a>. These can be combined with arbitrary weight
transformations to achieve a large family of associated
distributions. For example, to use a <a class="reference external" href="https://en.wikipedia.org/wiki/Log-normal_distribution">log-normal</a> weight model
for positive real weights <span class="math">\(\boldsymbol x\)</span>, we can use the
transformation <span class="math">\(y_{ij} = \ln x_{ij}\)</span> together with the
<code class="docutils literal notranslate"><span class="pre">&quot;real-normal&quot;</span></code> model for <span class="math">\(\boldsymbol y\)</span>. To model weights that
are positive or negative integers in <span class="math">\(\mathbb{Z}\)</span>, we could either
subtract the minimum value, <span class="math">\(y_{ij} = x_{ij} - x^*\)</span>, with
<span class="math">\(x^*=\operatorname{min}_{ij}x_{ij}\)</span>, and use any of the above
models for non-negative integers in <span class="math">\(\mathbb{N}\)</span>, or
alternatively, consider the sign as an additional covariate,
i.e. <span class="math">\(s_{ij} = [\operatorname{sign}(x_{ij})+1]/2 \in \{0,1\}\)</span>,
using the Binomial distribution with <span class="math">\(M=1\)</span> (a.k.a. the <a class="reference external" href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli
distribution</a>),
and any of the other discrete distributions for the magnitude,
<span class="math">\(y_{ij} = \operatorname{abs}(x_{ij})\)</span>.</p>
<p>The support for weighted networks is activated by passing the parameters
<code class="docutils literal notranslate"><span class="pre">recs</span></code> and <code class="docutils literal notranslate"><span class="pre">rec_types</span></code> to
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState" title="graph_tool.inference.blockmodel.BlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlockState</span></code></a> (or
<a class="reference internal" href="../../inference.html#graph_tool.inference.overlap_blockmodel.OverlapBlockState" title="graph_tool.inference.overlap_blockmodel.OverlapBlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">OverlapBlockState</span></code></a>),
that specify the edge covariates (an edge
<a class="reference internal" href="../../graph_tool.html#graph_tool.PropertyMap" title="graph_tool.PropertyMap"><code class="xref py py-class docutils literal notranslate"><span class="pre">PropertyMap</span></code></a>) and their types (a string from the
table above), respectively. Note that these parameters expect <em>lists</em>,
so that multiple edge weights can be used simultaneously.</p>
<p>For example, let us consider a network of suspected terrorists involved
in the train bombing of Madrid on March 11, 2004
<a class="reference internal" href="inference.html#hayes-connecting-2006" id="id23">[hayes-connecting-2006]</a>. An edge indicates that a connection between
the two persons have been identified, and the weight of the edge (an
integer in the range <span class="math">\([0,3]\)</span>) indicates the “strength” of the
connection. We can apply the weighted SBM, using a Binomial model for
the weights, as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">konect_data</span><span class="p">[</span><span class="s2">&quot;moreno_train&quot;</span><span class="p">]</span>

<span class="c1"># This network contains an internal edge property map with name</span>
<span class="c1"># &quot;weight&quot; that contains the strength of interactions. The values</span>
<span class="c1"># integers in the range [0, 3].</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">minimize_nested_blockmodel_dl</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">state_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">recs</span><span class="o">=</span><span class="p">[</span><span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="p">],</span>
                                                            <span class="n">rec_types</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;discrete-binomial&quot;</span><span class="p">]))</span>

<span class="n">state</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">edge_color</span><span class="o">=</span><span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">ecmap</span><span class="o">=</span><span class="p">(</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span><span class="p">,</span> <span class="o">.</span><span class="mi">6</span><span class="p">),</span>
           <span class="n">eorder</span><span class="o">=</span><span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">edge_pen_width</span><span class="o">=</span><span class="n">gt</span><span class="o">.</span><span class="n">prop_to_size</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
           <span class="n">edge_gradient</span><span class="o">=</span><span class="p">[],</span> <span class="n">output</span><span class="o">=</span><span class="s2">&quot;moreno-train-wsbm.svg&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center" id="id49">
<a class="reference internal image-reference" href="../../_images/moreno-train-wsbm.svg"><img alt="../../_images/moreno-train-wsbm.svg" src="../../_images/moreno-train-wsbm.svg" width="350px" /></a>
<p class="caption"><span class="caption-text">Best fit of the Binomial-weighted degree-corrected SBM for a network
of terror suspects, using the strength of connection as edge
covariates. The edge colors and widths correspond to the strengths.</span></p>
</div>
<div class="section" id="id24">
<h3>Model selection<a class="headerlink" href="inference.html#id24" title="Permalink to this headline">¶</a></h3>
<p>In order to select the best weighted model, we proceed in the same
manner as described in Sec. <a class="reference internal" href="inference.html#model-selection"><span class="std std-ref">Model selection</span></a>. However, when using
transformations on continuous weights, we must include the associated
scaling of the probability density, as described in
<a class="reference internal" href="inference.html#peixoto-weighted-2017" id="id25">[peixoto-weighted-2017]</a>.</p>
<p>For example, consider a <a class="reference external" href="https://en.wikipedia.org/wiki/Food_web">food web</a> between species in south
Florida <a class="reference internal" href="inference.html#ulanowicz-network-2005" id="id26">[ulanowicz-network-2005]</a>. A directed link exists from species
<span class="math">\(i\)</span> to <span class="math">\(j\)</span> if a energy flow exists between them, and a
weight <span class="math">\(x_{ij}\)</span> on this edge indicates the magnitude of the energy
flow (a positive real value, i.e. <span class="math">\(x_{ij}\in [0,\infty]\)</span>). One
possibility, therefore, is to use the <code class="docutils literal notranslate"><span class="pre">&quot;real-exponential&quot;</span></code> model, as
follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">konect_data</span><span class="p">[</span><span class="s2">&quot;foodweb-baywet&quot;</span><span class="p">]</span>

<span class="c1"># This network contains an internal edge property map with name</span>
<span class="c1"># &quot;weight&quot; that contains the biomass flow between species. The values</span>
<span class="c1"># are continuous in the range [0, infinity].</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">minimize_nested_blockmodel_dl</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">state_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">recs</span><span class="o">=</span><span class="p">[</span><span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="p">],</span>
                                                            <span class="n">rec_types</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;real-exponential&quot;</span><span class="p">]))</span>

<span class="n">state</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">edge_color</span><span class="o">=</span><span class="n">gt</span><span class="o">.</span><span class="n">prop_to_size</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">ecmap</span><span class="o">=</span><span class="p">(</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span><span class="p">,</span> <span class="o">.</span><span class="mi">6</span><span class="p">),</span>
           <span class="n">eorder</span><span class="o">=</span><span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">edge_pen_width</span><span class="o">=</span><span class="n">gt</span><span class="o">.</span><span class="n">prop_to_size</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
           <span class="n">edge_gradient</span><span class="o">=</span><span class="p">[],</span> <span class="n">output</span><span class="o">=</span><span class="s2">&quot;foodweb-wsbm.svg&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center" id="id50">
<a class="reference internal image-reference" href="../../_images/foodweb-wsbm.svg"><img alt="../../_images/foodweb-wsbm.svg" src="../../_images/foodweb-wsbm.svg" width="350px" /></a>
<p class="caption"><span class="caption-text">Best fit of the exponential-weighted degree-corrected SBM for a food
web, using the biomass flow as edge covariates (indicated by the edge
colors and widths).</span></p>
</div>
<p>Alternatively, we may consider a transformation of the type</p>
<div class="math" id="equation-log-transform">
<span class="eqno">(7)</span>\[y_{ij} = \ln x_{ij}\]</div>
<p>so that <span class="math">\(y_{ij} \in [-\infty,\infty]\)</span>. If we use a model
<code class="docutils literal notranslate"><span class="pre">&quot;real-normal&quot;</span></code> for <span class="math">\(\boldsymbol y\)</span>, it amounts to a <a class="reference external" href="https://en.wikipedia.org/wiki/Log-normal_distribution">log-normal</a> model for
<span class="math">\(\boldsymbol x\)</span>. This can be a better choice if the weights are
distributed across many orders of magnitude, or show multi-modality. We
can fit this alternative model simply by using the transformed weights:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Apply the weight transformation</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">y</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">a</span><span class="p">)</span>

<span class="n">state_ln</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">minimize_nested_blockmodel_dl</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">state_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">recs</span><span class="o">=</span><span class="p">[</span><span class="n">y</span><span class="p">],</span>
                                                               <span class="n">rec_types</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;real-normal&quot;</span><span class="p">]))</span>

<span class="n">state_ln</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">edge_color</span><span class="o">=</span><span class="n">gt</span><span class="o">.</span><span class="n">prop_to_size</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">ecmap</span><span class="o">=</span><span class="p">(</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">inferno</span><span class="p">,</span> <span class="o">.</span><span class="mi">6</span><span class="p">),</span>
              <span class="n">eorder</span><span class="o">=</span><span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">edge_pen_width</span><span class="o">=</span><span class="n">gt</span><span class="o">.</span><span class="n">prop_to_size</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
              <span class="n">edge_gradient</span><span class="o">=</span><span class="p">[],</span> <span class="n">output</span><span class="o">=</span><span class="s2">&quot;foodweb-wsbm-lognormal.svg&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center" id="id51">
<a class="reference internal image-reference" href="../../_images/foodweb-wsbm-lognormal.svg"><img alt="../../_images/foodweb-wsbm-lognormal.svg" src="../../_images/foodweb-wsbm-lognormal.svg" width="350px" /></a>
<p class="caption"><span class="caption-text">Best fit of the log-normal-weighted degree-corrected SBM for a food
web, using the biomass flow as edge covariates (indicated by the edge
colors and widths).</span></p>
</div>
<p>At this point, we ask ourselves which of the above models yields the
best fit of the data. This is answered by performing model selection via
posterior odds ratios just like in Sec. <a class="reference internal" href="inference.html#model-selection"><span class="std std-ref">Model selection</span></a>. However,
here we need to take into account the scaling of the probability density
incurred by the variable transformation, i.e.</p>
<div class="math">
\[P(\boldsymbol x | \boldsymbol G, \boldsymbol b) =
P(\boldsymbol y(\boldsymbol x) | \boldsymbol G, \boldsymbol b)
\prod_{ij}\left[\frac{\mathrm{d}y_{ij}}{\mathrm{d}x_{ij}}(x_{ij})\right]^{A_{ij}}.\]</div>
<p>In the particular case of Eq. <a class="reference internal" href="inference.html#equation-log-transform">(7)</a>, we have</p>
<div class="math">
\[\prod_{ij}\left[\frac{\mathrm{d}y_{ij}}{\mathrm{d}x_{ij}}(x_{ij})\right]^{A_{ij}}
= \prod_{ij}\frac{1}{x_{ij}^{A_{ij}}}.\]</div>
<p>Therefore, we can compute the posterior odds ratio between both models as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">L1</span> <span class="o">=</span> <span class="o">-</span><span class="n">state</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span>
<span class="n">L2</span> <span class="o">=</span> <span class="o">-</span><span class="n">state_ln</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span> <span class="o">-</span> <span class="n">log</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="sa">u</span><span class="s2">&quot;ln </span><span class="se">\u039b</span><span class="s2">: &quot;</span><span class="p">,</span> <span class="n">L2</span> <span class="o">-</span> <span class="n">L1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>ln Λ:  -70.145685...
</pre></div>
</div>
<p>A value of <span class="math">\(\Lambda \approx \mathrm{e}^{-70} \approx 10^{-30}\)</span> in
favor the exponential model indicates that the log-normal model does not
provide a better fit for this particular data. Based on this, we
conclude that the exponential model should be preferred in this case.</p>
</div>
<div class="section" id="posterior-sampling">
<h3>Posterior sampling<a class="headerlink" href="inference.html#posterior-sampling" title="Permalink to this headline">¶</a></h3>
<p>The procedure to sample from the posterior distribution is identical to
what is described in Sec. <a class="reference internal" href="inference.html#sampling"><span class="std std-ref">Sampling from the posterior distribution</span></a>, but with the appropriate
initialization, i.e.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">state</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">BlockState</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">B</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">recs</span><span class="o">=</span><span class="p">[</span><span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="p">],</span> <span class="n">rec_types</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;discrete-poisson&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>or for the nested model</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">state</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">NestedBlockState</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">g</span><span class="o">.</span><span class="n">num_vertices</span><span class="p">())]</span> <span class="o">+</span> <span class="p">[</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">10</span><span class="p">,</span>
                            <span class="n">state_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">recs</span><span class="o">=</span><span class="p">[</span><span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="p">],</span>
                                            <span class="n">rec_types</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;discrete-poisson&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="layered-networks">
<h2>Layered networks<a class="headerlink" href="inference.html#layered-networks" title="Permalink to this headline">¶</a></h2>
<p>The edges of the network may be distributed in discrete “layers”,
representing distinct types if interactions
<a class="reference internal" href="inference.html#peixoto-inferring-2015" id="id28">[peixoto-inferring-2015]</a>. Extensions to the SBM may be defined for such
data, and they can be inferred using the exact same interface shown
above, except one should use the
<a class="reference internal" href="../../inference.html#graph_tool.inference.layered_blockmodel.LayeredBlockState" title="graph_tool.inference.layered_blockmodel.LayeredBlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayeredBlockState</span></code></a>
class, instead of
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState" title="graph_tool.inference.blockmodel.BlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">BlockState</span></code></a>. This class takes
two additional parameters: the <code class="docutils literal notranslate"><span class="pre">ec</span></code> parameter, that must correspond to
an edge <a class="reference internal" href="../../graph_tool.html#graph_tool.PropertyMap" title="graph_tool.PropertyMap"><code class="xref py py-class docutils literal notranslate"><span class="pre">PropertyMap</span></code></a> with the layer/covariate values
on the edges, and the Boolean <code class="docutils literal notranslate"><span class="pre">layers</span></code> parameter, which if <code class="docutils literal notranslate"><span class="pre">True</span></code>
specifies a layered model, otherwise one with categorical edge
covariates (not to be confused with the weighted models in
Sec. <a class="reference internal" href="inference.html#weights"><span class="std std-ref">Edge weights and covariates</span></a>).</p>
<p>If we use <a class="reference internal" href="../../inference.html#graph_tool.inference.minimize.minimize_blockmodel_dl" title="graph_tool.inference.minimize.minimize_blockmodel_dl"><code class="xref py py-func docutils literal notranslate"><span class="pre">minimize_blockmodel_dl()</span></code></a>, this can
be achieved simply by passing the option <code class="docutils literal notranslate"><span class="pre">layers=True</span></code> as well as the
appropriate value of <code class="docutils literal notranslate"><span class="pre">state_args</span></code>, which will be propagated to
<a class="reference internal" href="../../inference.html#graph_tool.inference.layered_blockmodel.LayeredBlockState" title="graph_tool.inference.layered_blockmodel.LayeredBlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">LayeredBlockState</span></code></a>’s constructor.</p>
<p>As an example, let us consider a social network of tribes, where two
types of interactions were recorded, amounting to either friendship or
enmity <a class="reference internal" href="inference.html#read-cultures-1954" id="id29">[read-cultures-1954]</a>. We may apply the layered model by
separating these two types of interactions in two layers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">konect_data</span><span class="p">[</span><span class="s2">&quot;ucidata-gama&quot;</span><span class="p">]</span>

<span class="c1"># The edge types are stored in the edge property map &quot;weights&quot;.</span>

<span class="c1"># Note the different meanings of the two &#39;layers&#39; parameters below: The</span>
<span class="c1"># first enables the use of LayeredBlockState, and the second selects</span>
<span class="c1"># the &#39;edge layers&#39; version (instead of &#39;edge covariates&#39;).</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">minimize_nested_blockmodel_dl</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                         <span class="n">state_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">ec</span><span class="o">=</span><span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>

<span class="n">state</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">edge_color</span><span class="o">=</span><span class="n">g</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">edge_gradient</span><span class="o">=</span><span class="p">[],</span>
           <span class="n">ecmap</span><span class="o">=</span><span class="p">(</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm_r</span><span class="p">,</span> <span class="o">.</span><span class="mi">6</span><span class="p">),</span> <span class="n">edge_pen_width</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
           <span class="n">output</span><span class="o">=</span><span class="s2">&quot;tribes-sbm-edge-layers.svg&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center" id="id52">
<a class="reference internal image-reference" href="../../_images/tribes-sbm-edge-layers.svg"><img alt="../../_images/tribes-sbm-edge-layers.svg" src="../../_images/tribes-sbm-edge-layers.svg" width="350px" /></a>
<p class="caption"><span class="caption-text">Best fit of the degree-corrected SBM with edge layers for a network
of tribes, with edge layers shown as colors. The groups show two
enemy tribes.</span></p>
</div>
<p>It is possible to perform model averaging of all layered variants
exactly like for the regular SBMs as was shown above.</p>
</div>
<div class="section" id="network-reconstruction">
<h2>Network reconstruction<a class="headerlink" href="inference.html#network-reconstruction" title="Permalink to this headline">¶</a></h2>
<p>An important application of generative models is to be able to
generalize from observations and make predictions that go beyond what is
seen in the data. This is particularly useful when the network we
observe is incomplete, or contains errors, i.e. some of the edges are
either missing or are outcomes of mistakes in measurement. In this
situation, we can use statistical inference to reconstruct the original
network. Following <a class="reference internal" href="inference.html#peixoto-reconstructing-2018" id="id30">[peixoto-reconstructing-2018]</a>, if
<span class="math">\(\boldsymbol{\mathcal{D}}\)</span> is the observed data, the network can
be reconstructed according to the posterior distribution,</p>
<div class="math">
\[P(\boldsymbol A, \boldsymbol b | \boldsymbol{\mathcal{D}}) =
\frac{P(\boldsymbol{\mathcal{D}} | \boldsymbol A)P(\boldsymbol A, \boldsymbol b)}{P(\boldsymbol{\mathcal{D}})}\]</div>
<p>where the likelihood <span class="math">\(P(\boldsymbol{\mathcal{D}}|\boldsymbol A)\)</span>
models the measurement process, and for the prior <span class="math">\(P(\boldsymbol
A, \boldsymbol b)\)</span> we use the SBM as before. This means that when
performing reconstruction, we sample both the community structure
<span class="math">\(\boldsymbol b\)</span> and the network <span class="math">\(\boldsymbol A\)</span> itself from
the posterior distribution. From it, we can obtain the marginal probability
of each edge,</p>
<div class="math">
\[\pi_{ij} = \sum_{\boldsymbol A, \boldsymbol b}A_{ij}P(\boldsymbol A, \boldsymbol b | \boldsymbol{\mathcal{D}}).\]</div>
<p>Based on the marginal posterior probabilities, the best estimate for the
whole underlying network <span class="math">\(\boldsymbol{\hat{A}}\)</span> is given by the
maximum of this distribution,</p>
<div class="math">
\[\begin{split}\hat A_{ij} =
    \begin{cases}
        1 &amp; \text{ if } \pi_{ij} &gt; \frac{1}{2},\\
        0 &amp; \text{ if } \pi_{ij} &lt; \frac{1}{2}.\\
    \end{cases}\end{split}\]</div>
<p>We can also make estimates <span class="math">\(\hat y\)</span> of arbitrary scalar network
properties <span class="math">\(y(\boldsymbol A)\)</span> via posterior averages,</p>
<div class="math">
\[\begin{split}\begin{align}
    \hat y &amp;= \sum_{\boldsymbol A, \boldsymbol b}y(\boldsymbol A)P(\boldsymbol A, \boldsymbol b | \boldsymbol{\mathcal{D}}),\\
    \sigma^2_y &amp;= \sum_{\boldsymbol A, \boldsymbol b}(y(\boldsymbol A)-\hat y)^2P(\boldsymbol A, \boldsymbol b | \boldsymbol{\mathcal{D}})
\end{align}\end{split}\]</div>
<p>with uncertainty given by <span class="math">\(\sigma_y\)</span>. This is gives us a complete
probabilistic reconstruction framework that fully reflects both the
information and the uncertainty in the measurement data. Furthermore,
the use of the SBM means that the reconstruction can take advantage of
the <em>correlations</em> observed in the data to further inform it, which
generally can lead to substantial improvements
<a class="reference internal" href="inference.html#peixoto-reconstructing-2018" id="id31">[peixoto-reconstructing-2018]</a>.</p>
<p>In graph-tool there is support for reconstruction with the above
framework for three measurement processes: 1. Repeated measurements with
uniform errors (via
<a class="reference internal" href="../../inference.html#graph_tool.inference.uncertain_blockmodel.MeasuredBlockState" title="graph_tool.inference.uncertain_blockmodel.MeasuredBlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">MeasuredBlockState</span></code></a>), 2. Repeated
measurements with heterogeneous errors (via
<a class="reference internal" href="../../inference.html#graph_tool.inference.uncertain_blockmodel.MixedMeasuredBlockState" title="graph_tool.inference.uncertain_blockmodel.MixedMeasuredBlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">MixedMeasuredBlockState</span></code></a>),
and 3. Extraneously obtained edge probabilities (via
<a class="reference internal" href="../../inference.html#graph_tool.inference.uncertain_blockmodel.UncertainBlockState" title="graph_tool.inference.uncertain_blockmodel.UncertainBlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">UncertainBlockState</span></code></a>),
which we describe in the following.</p>
<div class="section" id="measured-networks">
<h3>Measured networks<a class="headerlink" href="inference.html#measured-networks" title="Permalink to this headline">¶</a></h3>
<p>This model assumes that the node pairs <span class="math">\((i,j)\)</span> were measured
<span class="math">\(n_{ij}\)</span> times, and an edge has been recorded <span class="math">\(x_{ij}\)</span>
times, where a missing edge occurs with probability <span class="math">\(p\)</span> and a
spurious edge occurs with probability <span class="math">\(q\)</span>, uniformly for all node
pairs, yielding a likelihood</p>
<div class="math">
\[\begin{split}P(\boldsymbol x | \boldsymbol n, \boldsymbol A, p, q) =
\prod_{i&lt;j}{n_{ij}\choose x_{ij}}\left[(1-p)^{x_{ij}}p^{n_{ij}-x_{ij}}\right]^{A_{ij}}
\left[q^{x_{ij}}(1-q)^{n_{ij}-x_{ij}}\right]^{1-A_{ij}}.\end{split}\]</div>
<p>In general, <span class="math">\(p\)</span> and <span class="math">\(q\)</span> are not precisely known <em>a priori</em>,
so we consider the integrated likelihood</p>
<div class="math">
\[P(\boldsymbol x | \boldsymbol n, \boldsymbol A, \alpha,\beta,\mu,\nu) =
\int P(\boldsymbol x | \boldsymbol n, \boldsymbol A, p, q) P(p|\alpha,\beta) P(q|\mu,\nu)\;\mathrm{d}p\,\mathrm{d}q\]</div>
<p>where <span class="math">\(P(p|\alpha,\beta)\)</span> and <span class="math">\(P(q|\mu,\nu)\)</span> are <a class="reference external" href="https://en.wikipedia.org/wiki/Beta_distribution">Beta
distributions</a>, which
specify the amount of prior knowledge we have on the noise
parameters. An important special case, which is the default unless
otherwise specified, is when we are completely agnostic <em>a priori</em> about
the noise magnitudes, and all hyperparameters are unity,</p>
<div class="math">
\[P(\boldsymbol x | \boldsymbol n, \boldsymbol A) \equiv
P(\boldsymbol x | \boldsymbol n, \boldsymbol A, \alpha=1,\beta=1,\mu=1,\nu=1).\]</div>
<p>In this situation the priors <span class="math">\(P(p|\alpha=1,\beta=1)\)</span> and
<span class="math">\(P(q|\mu=1,\nu=1)\)</span> are uniform distribution in the interval <span class="math">\([0,1]\)</span>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">It is important to emphasize that since this approach makes use of
the <em>correlations</em> between edges to inform the reconstruction, as
described by the inferred SBM, this means it can also be used when
only single measurements have been performed, <span class="math">\(n_{ij}=1\)</span>, and
the error magnitudes <span class="math">\(p\)</span> and <span class="math">\(q\)</span> are unknown. Since every
arbitrary adjacency matrix can be cast in this setting, this method
can be used to reconstruct networks for which no error assessments of
any kind have been provided.</p>
</div>
<p>Below, we illustrate how the reconstruction can be performed with a
simple example, using
<a class="reference internal" href="../../inference.html#graph_tool.inference.uncertain_blockmodel.MeasuredBlockState" title="graph_tool.inference.uncertain_blockmodel.MeasuredBlockState"><code class="xref py py-class docutils literal notranslate"><span class="pre">MeasuredBlockState</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;lesmis&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># pretend we have measured and observed each edge twice</span>

<span class="n">n</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">new_ep</span><span class="p">(</span><span class="s2">&quot;int&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>   <span class="c1"># number of measurements</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">new_ep</span><span class="p">(</span><span class="s2">&quot;int&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>   <span class="c1"># number of observations</span>

<span class="n">e</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">36</span><span class="p">)</span>
<span class="n">x</span><span class="p">[</span><span class="n">e</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>                 <span class="c1"># pretend we have observed edge (11, 36) only once</span>

<span class="n">e</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">73</span><span class="p">)</span>
<span class="n">n</span><span class="p">[</span><span class="n">e</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>                 <span class="c1"># pretend we have measured non-edge (15, 73) twice,</span>
<span class="n">x</span><span class="p">[</span><span class="n">e</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>                 <span class="c1"># but observed it as an edge once.</span>

<span class="n">bs</span> <span class="o">=</span> <span class="p">[</span><span class="n">g</span><span class="o">.</span><span class="n">get_vertices</span><span class="p">()]</span> <span class="o">+</span> <span class="p">[</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">5</span>  <span class="c1"># initial hierarchical partition</span>

<span class="c1"># We inititialize MeasuredBlockState, assuming that each non-edge has</span>
<span class="c1"># been measured only once (as opposed to twice for the observed</span>
<span class="c1"># edges), as specified by the &#39;n_default&#39; and &#39;x_default&#39; parameters.</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">MeasuredBlockState</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">n_default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">x_default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                              <span class="n">state_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">))</span>

<span class="c1"># We will first equilibrate the Markov chain</span>
<span class="n">gt</span><span class="o">.</span><span class="n">mcmc_equilibrate</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">wait</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">mcmc_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

<span class="c1"># Now we collect the marginals for exactly 100,000 sweeps, at</span>
<span class="c1"># intervals of 10 sweeps:</span>

<span class="n">u</span> <span class="o">=</span> <span class="bp">None</span>              <span class="c1"># marginal posterior edge probabilities</span>
<span class="n">pv</span> <span class="o">=</span> <span class="bp">None</span>             <span class="c1"># marginal posterior group membership probabilities</span>
<span class="n">cs</span> <span class="o">=</span> <span class="p">[]</span>               <span class="c1"># average local clustering coefficient</span>

<span class="k">def</span> <span class="nf">collect_marginals</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
   <span class="k">global</span> <span class="n">pv</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">cs</span>
   <span class="n">u</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">collect_marginal</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
   <span class="n">bstate</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">get_block_state</span><span class="p">()</span>
   <span class="n">pv</span> <span class="o">=</span> <span class="n">bstate</span><span class="o">.</span><span class="n">levels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">collect_vertex_marginals</span><span class="p">(</span><span class="n">pv</span><span class="p">)</span>
   <span class="n">cs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gt</span><span class="o">.</span><span class="n">local_clustering</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">get_graph</span><span class="p">())</span><span class="o">.</span><span class="n">fa</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="n">gt</span><span class="o">.</span><span class="n">mcmc_equilibrate</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">force_niter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">mcmc_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                    <span class="n">callback</span><span class="o">=</span><span class="n">collect_marginals</span><span class="p">)</span>

<span class="n">eprob</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">eprob</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Posterior probability of edge (11, 36):&quot;</span><span class="p">,</span> <span class="n">eprob</span><span class="p">[</span><span class="n">u</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">36</span><span class="p">)])</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Posterior probability of non-edge (15, 73):&quot;</span><span class="p">,</span> <span class="n">eprob</span><span class="p">[</span><span class="n">u</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">73</span><span class="p">)])</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Estimated average local clustering: </span><span class="si">%g</span><span class="s2"> ± </span><span class="si">%g</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cs</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">cs</span><span class="p">)))</span>
</pre></div>
</div>
<p>Which yields the following output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Posterior probability of edge (11, 36): 0.801980198019802
Posterior probability of non-edge (15, 73): 0.09730973097309731
Estimated average local clustering: 0.572154 ± 0.00485314
</pre></div>
</div>
<p>We have a successful reconstruction, where both ambiguous adjacency
matrix entries are correctly recovered. The value for the average
clustering coefficient is also correctly estimated, and is compatible
with the true value <span class="math">\(0.57313675\)</span>, within the estimated error.</p>
<p>Below we visualize the maximum marginal posterior estimate of the
reconstructed network:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The maximum marginal posterior estimator can be obtained by</span>
<span class="c1"># filtering the edges with probability larger than .5</span>

<span class="n">u</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">GraphView</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">efilt</span><span class="o">=</span><span class="n">u</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">eprob</span><span class="o">.</span><span class="n">fa</span> <span class="o">&gt;</span> <span class="o">.</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Mark the recovered true edges as red, and the removed spurious edges as green</span>
<span class="n">ecolor</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">new_ep</span><span class="p">(</span><span class="s2">&quot;vector&lt;double&gt;&quot;</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">6</span><span class="p">])</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">u</span><span class="o">.</span><span class="n">edges</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">g</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">source</span><span class="p">(),</span> <span class="n">e</span><span class="o">.</span><span class="n">target</span><span class="p">())</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">source</span><span class="p">(),</span> <span class="n">e</span><span class="o">.</span><span class="n">target</span><span class="p">())</span> <span class="o">==</span> <span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">36</span><span class="p">):</span>
        <span class="n">ecolor</span><span class="p">[</span><span class="n">e</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">6</span><span class="p">]</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">g</span><span class="o">.</span><span class="n">edges</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">u</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">source</span><span class="p">(),</span> <span class="n">e</span><span class="o">.</span><span class="n">target</span><span class="p">())</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">ne</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">source</span><span class="p">(),</span> <span class="n">e</span><span class="o">.</span><span class="n">target</span><span class="p">())</span>
        <span class="n">ecolor</span><span class="p">[</span><span class="n">ne</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">6</span><span class="p">]</span>

<span class="c1"># Duplicate the internal block state with the reconstructed network</span>
<span class="c1"># u, for visualization purposes.</span>

<span class="n">bstate</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get_block_state</span><span class="p">()</span>
<span class="n">bstate</span> <span class="o">=</span> <span class="n">bstate</span><span class="o">.</span><span class="n">levels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">g</span><span class="o">=</span><span class="n">u</span><span class="p">)</span>

<span class="n">pv</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">own_property</span><span class="p">(</span><span class="n">pv</span><span class="p">)</span>
<span class="n">edash</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">new_ep</span><span class="p">(</span><span class="s2">&quot;vector&lt;double&gt;&quot;</span><span class="p">)</span>
<span class="n">edash</span><span class="p">[</span><span class="n">u</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">73</span><span class="p">)]</span> <span class="o">=</span> <span class="p">[</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">bstate</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">pos</span><span class="o">=</span><span class="n">u</span><span class="o">.</span><span class="n">own_property</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">vp</span><span class="o">.</span><span class="n">pos</span><span class="p">),</span> <span class="n">vertex_shape</span><span class="o">=</span><span class="s2">&quot;pie&quot;</span><span class="p">,</span> <span class="n">vertex_pie_fractions</span><span class="o">=</span><span class="n">pv</span><span class="p">,</span>
            <span class="n">edge_color</span><span class="o">=</span><span class="n">ecolor</span><span class="p">,</span> <span class="n">edge_dash_style</span><span class="o">=</span><span class="n">edash</span><span class="p">,</span> <span class="n">edge_gradient</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
            <span class="n">output</span><span class="o">=</span><span class="s2">&quot;lesmis-reconstruction-marginals.svg&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center" id="id53">
<a class="reference internal image-reference" href="../../_images/lesmis-reconstruction-marginals.svg"><img alt="../../_images/lesmis-reconstruction-marginals.svg" src="../../_images/lesmis-reconstruction-marginals.svg" width="450px" /></a>
<p class="caption"><span class="caption-text">Reconstructed network of characters in the novel Les Misérables,
assuming that each edge has been measured and recorded twice, and
each non-edge has been measured only once, with the exception of edge
(11, 36), shown in red, and non-edge (15, 73), shown in green, which
have been measured twice and recorded as an edge once. Despite the
ambiguity, both errors are successfully corrected by the
reconstruction. The pie fractions on the nodes correspond to the
probability of being in group associated with the respective color.</span></p>
</div>
<div class="section" id="heterogeneous-errors">
<h4>Heterogeneous errors<a class="headerlink" href="inference.html#heterogeneous-errors" title="Permalink to this headline">¶</a></h4>
<p>In a more general scenario the measurement errors can be different for
each node pair, i.e. <span class="math">\(p_{ij}\)</span> and <span class="math">\(q_{ij}\)</span> are the missing
and spurious edge probability for node pair <span class="math">\((i,j)\)</span>. The
measurement likelihood then becomes</p>
<div class="math">
\[\begin{split}P(\boldsymbol x | \boldsymbol n, \boldsymbol A, \boldsymbol p, \boldsymbol q) =
\prod_{i&lt;j}{n_{ij}\choose x_{ij}}\left[(1-p_{ij})^{x_{ij}}p_{ij}^{n_{ij}-x_{ij}}\right]^{A_{ij}}
\left[q_{ij}^{x_{ij}}(1-q_{ij})^{n_{ij}-x_{ij}}\right]^{1-A_{ij}}.\end{split}\]</div>
<p>Since the noise magnitudes are <em>a priori</em> unknown, we consider the
integrated likelihood</p>
<div class="math">
\[\begin{split}P(\boldsymbol x | \boldsymbol n, \boldsymbol A, \alpha,\beta,\mu,\nu) =
\prod_{i&lt;j}\int P(x_{ij} | n_{ij}, A_{ij}, p_{ij}, q_{ij}) P(p_{ij}|\alpha,\beta) P(q_{ij}|\mu,\nu)\;\mathrm{d}p_{ij}\,\mathrm{d}q_{ij}\end{split}\]</div>
<p>where <span class="math">\(P(p_{ij}|\alpha,\beta)\)</span> and <span class="math">\(P(q_{ij}|\mu,\nu)\)</span> are
<a class="reference external" href="https://en.wikipedia.org/wiki/Beta_distribution">Beta prior distributions</a>, like
before. Instead of pre-specifying the hyperparameters, we include them
from the posterior distribution</p>
<div class="math">
\[P(\boldsymbol A, \boldsymbol b, \alpha,\beta,\mu,\nu | \boldsymbol x, \boldsymbol n) =
\frac{P(\boldsymbol x | \boldsymbol n, \boldsymbol A, \alpha,\beta,\mu,\nu)P(\boldsymbol A, \boldsymbol b)P(\alpha,\beta,\mu,\nu)}{P(\boldsymbol x| \boldsymbol n)},\]</div>
<p>where <span class="math">\(P(\alpha,\beta,\mu,\nu)\propto 1\)</span> is a uniform hyperprior.</p>
<p>Operationally, the inference with this model works similarly to the one
with uniform error rates, as we see with the same example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">state</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">MixedMeasuredBlockState</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">n_default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">x_default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                   <span class="n">state_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">))</span>

<span class="c1"># We will first equilibrate the Markov chain</span>
<span class="n">gt</span><span class="o">.</span><span class="n">mcmc_equilibrate</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">wait</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">mcmc_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

<span class="c1"># Now we collect the marginals for exactly 100,000 sweeps, at</span>
<span class="c1"># intervals of 10 sweeps:</span>

<span class="n">u</span> <span class="o">=</span> <span class="bp">None</span>              <span class="c1"># marginal posterior edge probabilities</span>
<span class="n">pv</span> <span class="o">=</span> <span class="bp">None</span>             <span class="c1"># marginal posterior group membership probabilities</span>
<span class="n">cs</span> <span class="o">=</span> <span class="p">[]</span>               <span class="c1"># average local clustering coefficient</span>

<span class="n">gt</span><span class="o">.</span><span class="n">mcmc_equilibrate</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">force_niter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">mcmc_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                    <span class="n">callback</span><span class="o">=</span><span class="n">collect_marginals</span><span class="p">)</span>

<span class="n">eprob</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">eprob</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Posterior probability of edge (11, 36):&quot;</span><span class="p">,</span> <span class="n">eprob</span><span class="p">[</span><span class="n">u</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">36</span><span class="p">)])</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Posterior probability of non-edge (15, 73):&quot;</span><span class="p">,</span> <span class="n">eprob</span><span class="p">[</span><span class="n">u</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">73</span><span class="p">)])</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Estimated average local clustering: </span><span class="si">%g</span><span class="s2"> ± </span><span class="si">%g</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cs</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">cs</span><span class="p">)))</span>
</pre></div>
</div>
<p>Which yields:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Posterior probability of edge (11, 36): 0.7901790179017901
Posterior probability of non-edge (15, 73): 0.10901090109010901
Estimated average local clustering: 0.572504 ± 0.00545337
</pre></div>
</div>
<p>The results are very similar to the ones obtained with the uniform model
in this case, but can be quite different in situations where a large
number of measurements has been performed (see
<a class="reference internal" href="inference.html#peixoto-reconstructing-2018" id="id32">[peixoto-reconstructing-2018]</a> for details).</p>
</div>
</div>
<div class="section" id="extraneous-error-estimates">
<h3>Extraneous error estimates<a class="headerlink" href="inference.html#extraneous-error-estimates" title="Permalink to this headline">¶</a></h3>
<p>In some situations the edge uncertainties are estimated by means other
than repeated measurements, using domain-specific models. Here we
consider the general case where the error estimates are extraneously
provided as independent edge probabilities <span class="math">\(\boldsymbol Q\)</span>,</p>
<div class="math">
\[\begin{split}P_Q(\boldsymbol A | \boldsymbol Q) = \prod_{i&lt;j}Q_{ij}^{A_{ij}}(1-Q_{ij})^{1-A_{ij}},\end{split}\]</div>
<p>where <span class="math">\(Q_{ij}\)</span> is the estimated probability of edge
<span class="math">\((i,j)\)</span>. Although in principle we could reconstruct networks
directly from the above distribution, we can also incorporate it with
SBM inference to take advantage of large-scale structures present in the
data. We do so by employing Bayes’ rule to extract the noise model from
the provided values <a class="reference internal" href="inference.html#martin-structural-2015" id="id33">[martin-structural-2015]</a>
<a class="reference internal" href="inference.html#peixoto-reconstructing-2018" id="id34">[peixoto-reconstructing-2018]</a>,</p>
<div class="math">
\[\begin{split}\begin{align}
    P_Q(\boldsymbol Q | \boldsymbol A) &amp;= \frac{P_Q(\boldsymbol A | \boldsymbol Q)P_Q(\boldsymbol Q)}{P_Q(\boldsymbol A)},\\
    &amp; = P_Q(\boldsymbol Q) \prod_{i&lt;j} \left(\frac{Q_{ij}}{\bar Q}\right)^{A_{ij}}\left(\frac{1-Q_{ij}}{1-\bar Q}\right)^{1-A_{ij}},
\end{align}\end{split}\]</div>
<p>where <span class="math">\(\bar Q = \sum_{i&lt;j}Q_{ij}/{N\choose 2}\)</span> is the estimated
network density, and <span class="math">\(P_Q(\boldsymbol Q)\)</span> is an unknown prior for
<span class="math">\(\boldsymbol Q\)</span>, which can remain unspecified as it has no effect
on the posterior distribution. With the above, we can reconstruct the
network based on the posterior distribution,</p>
<div class="math">
\[P(\boldsymbol A, \boldsymbol b | \boldsymbol Q) = \frac{P_Q(\boldsymbol Q | \boldsymbol A)P(\boldsymbol A, \boldsymbol b)}{P(\boldsymbol Q)}\]</div>
<p>where <span class="math">\(P(\boldsymbol A, \boldsymbol b)\)</span> is the joint SBM
distribution used before. Note that this reconstruction will be
different from the one obtained directly from the original estimation, i.e.</p>
<div class="math">
\[P(\boldsymbol A | \boldsymbol Q) = \sum_{\boldsymbol b}P(\boldsymbol A, \boldsymbol b | \boldsymbol Q) \neq P_Q(\boldsymbol A | \boldsymbol Q).\]</div>
<p>This is because the posterior <span class="math">\(P(\boldsymbol A | \boldsymbol Q)\)</span>
will take into consideration the correlations found in the data, as
captured by the inferred SBM structure, as further evidence for the
existence and non-existence of edges. We illustrate this with an example
similar to the one considered previously, where two adjacency matrix
entries with the same ambiguous edge probability <span class="math">\(Q_{ij}=1/2\)</span> are
correctly reconstructed as edge and non-edge, due to the joint SBM
inference:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;lesmis&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="n">N</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">num_vertices</span><span class="p">()</span>
<span class="n">E</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">num_edges</span><span class="p">()</span>

<span class="n">q</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">new_ep</span><span class="p">(</span><span class="s2">&quot;double&quot;</span><span class="p">,</span> <span class="o">.</span><span class="mi">98</span><span class="p">)</span>   <span class="c1"># edge uncertainties</span>

<span class="n">e</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">36</span><span class="p">)</span>
<span class="n">q</span><span class="p">[</span><span class="n">e</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span>                     <span class="c1"># ambiguous true edge</span>

<span class="n">e</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">73</span><span class="p">)</span>
<span class="n">q</span><span class="p">[</span><span class="n">e</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span>                     <span class="c1"># ambiguous spurious edge</span>

<span class="n">bs</span> <span class="o">=</span> <span class="p">[</span><span class="n">g</span><span class="o">.</span><span class="n">get_vertices</span><span class="p">()]</span> <span class="o">+</span> <span class="p">[</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">5</span>  <span class="c1"># initial hierarchical partition</span>

<span class="c1"># We inititialize UncertainBlockState, assuming that each non-edge</span>
<span class="c1"># has an uncertainty of q_default, chosen to preserve the expected</span>
<span class="c1"># density of the original network:</span>

<span class="n">q_default</span> <span class="o">=</span> <span class="p">(</span><span class="n">E</span> <span class="o">-</span> <span class="n">q</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span> <span class="o">/</span> <span class="p">((</span><span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="mi">2</span> <span class="o">-</span> <span class="n">E</span><span class="p">)</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">UncertainBlockState</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="n">q</span><span class="p">,</span> <span class="n">q_default</span><span class="o">=</span><span class="n">q_default</span><span class="p">,</span> <span class="n">state_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">))</span>

<span class="c1"># We will first equilibrate the Markov chain</span>
<span class="n">gt</span><span class="o">.</span><span class="n">mcmc_equilibrate</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">wait</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">mcmc_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>

<span class="c1"># Now we collect the marginals for exactly 100,000 sweeps, at</span>
<span class="c1"># intervals of 10 sweeps:</span>

<span class="n">u</span> <span class="o">=</span> <span class="bp">None</span>              <span class="c1"># marginal posterior edge probabilities</span>
<span class="n">pv</span> <span class="o">=</span> <span class="bp">None</span>             <span class="c1"># marginal posterior group membership probabilities</span>
<span class="n">cs</span> <span class="o">=</span> <span class="p">[]</span>               <span class="c1"># average local clustering coefficient</span>

<span class="k">def</span> <span class="nf">collect_marginals</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
   <span class="k">global</span> <span class="n">pv</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">cs</span>
   <span class="n">u</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">collect_marginal</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
   <span class="n">bstate</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">get_block_state</span><span class="p">()</span>
   <span class="n">pv</span> <span class="o">=</span> <span class="n">bstate</span><span class="o">.</span><span class="n">levels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">collect_vertex_marginals</span><span class="p">(</span><span class="n">pv</span><span class="p">)</span>
   <span class="n">cs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gt</span><span class="o">.</span><span class="n">local_clustering</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">get_graph</span><span class="p">())</span><span class="o">.</span><span class="n">fa</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="n">gt</span><span class="o">.</span><span class="n">mcmc_equilibrate</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">force_niter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">mcmc_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                    <span class="n">callback</span><span class="o">=</span><span class="n">collect_marginals</span><span class="p">)</span>

<span class="n">eprob</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">eprob</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Posterior probability of edge (11, 36):&quot;</span><span class="p">,</span> <span class="n">eprob</span><span class="p">[</span><span class="n">u</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">36</span><span class="p">)])</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Posterior probability of non-edge (15, 73):&quot;</span><span class="p">,</span> <span class="n">eprob</span><span class="p">[</span><span class="n">u</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">73</span><span class="p">)])</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Estimated average local clustering: </span><span class="si">%g</span><span class="s2"> ± </span><span class="si">%g</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cs</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">cs</span><span class="p">)))</span>
</pre></div>
</div>
<p>The above yields the output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Posterior probability of edge (11, 36): 0.9504950495049505
Posterior probability of non-edge (15, 73): 0.0674067406740674
Estimated average local clustering: 0.552333 ± 0.0191831
</pre></div>
</div>
<p>The reconstruction is accurate, despite the two ambiguous entries having
the same measurement probability. The reconstructed network is visualized below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># The maximum marginal posterior estimator can be obtained by</span>
<span class="c1"># filtering the edges with probability larger than .5</span>

<span class="n">u</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">GraphView</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">efilt</span><span class="o">=</span><span class="n">u</span><span class="o">.</span><span class="n">ep</span><span class="o">.</span><span class="n">eprob</span><span class="o">.</span><span class="n">fa</span> <span class="o">&gt;</span> <span class="o">.</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Mark the recovered true edges as red, and the removed spurious edges as green</span>
<span class="n">ecolor</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">new_ep</span><span class="p">(</span><span class="s2">&quot;vector&lt;double&gt;&quot;</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">6</span><span class="p">])</span>
<span class="n">edash</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">new_ep</span><span class="p">(</span><span class="s2">&quot;vector&lt;double&gt;&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">u</span><span class="o">.</span><span class="n">edges</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">g</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">source</span><span class="p">(),</span> <span class="n">e</span><span class="o">.</span><span class="n">target</span><span class="p">())</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">source</span><span class="p">(),</span> <span class="n">e</span><span class="o">.</span><span class="n">target</span><span class="p">())</span> <span class="o">==</span> <span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">36</span><span class="p">):</span>
        <span class="n">ecolor</span><span class="p">[</span><span class="n">e</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">6</span><span class="p">]</span>

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">g</span><span class="o">.</span><span class="n">edges</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">u</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">source</span><span class="p">(),</span> <span class="n">e</span><span class="o">.</span><span class="n">target</span><span class="p">())</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">ne</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">source</span><span class="p">(),</span> <span class="n">e</span><span class="o">.</span><span class="n">target</span><span class="p">())</span>
        <span class="n">ecolor</span><span class="p">[</span><span class="n">ne</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">6</span><span class="p">]</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">source</span><span class="p">(),</span> <span class="n">e</span><span class="o">.</span><span class="n">target</span><span class="p">())</span> <span class="o">==</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">73</span><span class="p">):</span>
            <span class="n">edash</span><span class="p">[</span><span class="n">ne</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="n">bstate</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get_block_state</span><span class="p">()</span>
<span class="n">bstate</span> <span class="o">=</span> <span class="n">bstate</span><span class="o">.</span><span class="n">levels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">g</span><span class="o">=</span><span class="n">u</span><span class="p">)</span>
<span class="n">pv</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">own_property</span><span class="p">(</span><span class="n">pv</span><span class="p">)</span>
<span class="n">bstate</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="n">pos</span><span class="o">=</span><span class="n">u</span><span class="o">.</span><span class="n">own_property</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">vp</span><span class="o">.</span><span class="n">pos</span><span class="p">),</span> <span class="n">vertex_shape</span><span class="o">=</span><span class="s2">&quot;pie&quot;</span><span class="p">,</span> <span class="n">vertex_pie_fractions</span><span class="o">=</span><span class="n">pv</span><span class="p">,</span>
            <span class="n">edge_color</span><span class="o">=</span><span class="n">ecolor</span><span class="p">,</span> <span class="n">edge_dash_style</span><span class="o">=</span><span class="n">edash</span><span class="p">,</span> <span class="n">edge_gradient</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
            <span class="n">output</span><span class="o">=</span><span class="s2">&quot;lesmis-uncertain-reconstruction-marginals.svg&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-center" id="id54">
<a class="reference internal image-reference" href="../../_images/lesmis-uncertain-reconstruction-marginals.svg"><img alt="../../_images/lesmis-uncertain-reconstruction-marginals.svg" src="../../_images/lesmis-uncertain-reconstruction-marginals.svg" width="450px" /></a>
<p class="caption"><span class="caption-text">Reconstructed network of characters in the novel Les Misérables,
assuming that each edge as a measurement probability of
<span class="math">\(.98\)</span>. Edge (11, 36), shown in red, and non-edge (15, 73),
shown in green, both have probability <span class="math">\(0.5\)</span>. Despite the
ambiguity, both errors are successfully corrected by the
reconstruction. The pie fractions on the nodes correspond to the
probability of being in group associated with the respective color.</span></p>
</div>
</div>
<div class="section" id="edge-prediction-as-binary-classification">
<h3>Edge prediction as binary classification<a class="headerlink" href="inference.html#edge-prediction-as-binary-classification" title="Permalink to this headline">¶</a></h3>
<p>A more traditional approach to the prediction of missing and spurious
edges formulates it as a supervised <a class="reference external" href="https://en.wikipedia.org/wiki/Binary_classification">binary classification task</a>, where the
edge/non-edge scores are computed by fitting a generative model to the
observed data, and computing their probabilities under that model
<a class="reference internal" href="inference.html#clauset-hierarchical-2008" id="id35">[clauset-hierarchical-2008]</a> <a class="reference internal" href="inference.html#guimera-missing-2009" id="id36">[guimera-missing-2009]</a>. In this setting,
one typically omits any explicit model of the measurement process (hence
intrinsically assuming it to be uniform), and as a consequence of the
overall setup, only <em>relative probabilities</em> between individual missing
and spurious edges can be produced, instead of the full posterior
distribution considered in the last section. Since this limits the
overall network reconstruction, and does not yields confidence
intervals, it is a less powerful approach. Nevertheless, it is a popular
procedure, which can also be performed with graph-tool, as we describe
in the following.</p>
<p>We set up the classification task by dividing the edges/non-edges into
two sets <span class="math">\(\boldsymbol G\)</span> and <span class="math">\(\delta \boldsymbol G\)</span>, where
the former corresponds to the observed network and the latter either to
the missing or spurious edges. We may compute the posterior of
<span class="math">\(\delta \boldsymbol G\)</span> as <a class="reference internal" href="inference.html#valles-catala-consistency-2017" id="id37">[valles-catala-consistency-2017]</a></p>
<div class="math" id="equation-posterior-missing">
<span class="eqno">(8)</span>\[P(\delta \boldsymbol G | \boldsymbol G) \propto
\sum_{\boldsymbol b}\frac{P(\boldsymbol G \cup \delta\boldsymbol G| \boldsymbol b)}{P(\boldsymbol G| \boldsymbol b)}P(\boldsymbol b | \boldsymbol G)\]</div>
<p>up to a normalization constant <a class="footnote-reference" href="inference.html#prediction-posterior" id="id38">[1]</a>. Although the
normalization constant is difficult to obtain in general (since we need
to perform a sum over all possible spurious/missing edges), the
numerator of Eq. <a class="reference internal" href="inference.html#equation-posterior-missing">(8)</a> can be computed by sampling
partitions from the posterior, and then inserting or deleting edges from
the graph and computing the new likelihood. This means that we can
easily compare alternative predictive hypotheses <span class="math">\(\{\delta
\boldsymbol G_i\}\)</span> via their likelihood ratios</p>
<div class="math">
\[\lambda_i = \frac{P(\delta \boldsymbol G_i | \boldsymbol G)}{\sum_j P(\delta \boldsymbol G_j | \boldsymbol G)}\]</div>
<p>which do not depend on the normalization constant.</p>
<p>The values <span class="math">\(P(\delta \boldsymbol G | \boldsymbol G, \boldsymbol b)\)</span>
can be computed with
<a class="reference internal" href="../../inference.html#graph_tool.inference.blockmodel.BlockState.get_edges_prob" title="graph_tool.inference.blockmodel.BlockState.get_edges_prob"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_edges_prob()</span></code></a>. Hence, we can
compute spurious/missing edge probabilities just as if we were
collecting marginal distributions when doing model averaging.</p>
<p>Below is an example for predicting the two following edges in the
football network, using the nested model (for which we need to replace
<span class="math">\(\boldsymbol b\)</span> by <span class="math">\(\{\boldsymbol b_l\}\)</span> in the equations
above).</p>
<div class="figure align-center" id="id55">
<a class="reference internal image-reference" href="../../_images/football_missing.svg"><img alt="../../_images/football_missing.svg" src="../../_images/football_missing.svg" width="350px" /></a>
<p class="caption"><span class="caption-text">Two non-existing edges in the football network (in red):
<span class="math">\((101,102)\)</span> in the middle, and <span class="math">\((17,56)\)</span> in the upper
right region of the figure.</span></p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">collection</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;football&quot;</span><span class="p">]</span>

<span class="n">missing_edges</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">101</span><span class="p">,</span> <span class="mi">102</span><span class="p">),</span> <span class="p">(</span><span class="mi">17</span><span class="p">,</span> <span class="mi">56</span><span class="p">)]</span>

<span class="n">L</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">gt</span><span class="o">.</span><span class="n">minimize_nested_blockmodel_dl</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">deg_corr</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">bs</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get_bs</span><span class="p">()</span>                     <span class="c1"># Get hierarchical partition.</span>
<span class="n">bs</span> <span class="o">+=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)]</span> <span class="o">*</span> <span class="p">(</span><span class="n">L</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">bs</span><span class="p">))</span>     <span class="c1"># Augment it to L = 10 with</span>
                                        <span class="c1"># single-group levels.</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">bs</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">sampling</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">probs</span> <span class="o">=</span> <span class="p">([],</span> <span class="p">[])</span>

<span class="k">def</span> <span class="nf">collect_edge_probs</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">p1</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">get_edges_prob</span><span class="p">([</span><span class="n">missing_edges</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">entropy_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">partition_dl</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
    <span class="n">p2</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">get_edges_prob</span><span class="p">([</span><span class="n">missing_edges</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="n">entropy_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">partition_dl</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
    <span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p1</span><span class="p">)</span>
    <span class="n">probs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p2</span><span class="p">)</span>

<span class="c1"># Now we collect the probabilities for exactly 100,000 sweeps</span>
<span class="n">gt</span><span class="o">.</span><span class="n">mcmc_equilibrate</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">force_niter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">mcmc_args</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">niter</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                    <span class="n">callback</span><span class="o">=</span><span class="n">collect_edge_probs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_avg</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
   <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
   <span class="n">pmax</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
   <span class="n">p</span> <span class="o">-=</span> <span class="n">pmax</span>
   <span class="k">return</span> <span class="n">pmax</span> <span class="o">+</span> <span class="n">log</span><span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="n">p1</span> <span class="o">=</span> <span class="n">get_avg</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">p2</span> <span class="o">=</span> <span class="n">get_avg</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="n">p_sum</span> <span class="o">=</span> <span class="n">get_avg</span><span class="p">([</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">])</span> <span class="o">+</span> <span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="n">l1</span> <span class="o">=</span> <span class="n">p1</span> <span class="o">-</span> <span class="n">p_sum</span>
<span class="n">l2</span> <span class="o">=</span> <span class="n">p2</span> <span class="o">-</span> <span class="n">p_sum</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;likelihood-ratio for </span><span class="si">%s</span><span class="s2">: </span><span class="si">%g</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">missing_edges</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">exp</span><span class="p">(</span><span class="n">l1</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;likelihood-ratio for </span><span class="si">%s</span><span class="s2">: </span><span class="si">%g</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">missing_edges</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">exp</span><span class="p">(</span><span class="n">l2</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>likelihood-ratio for (101, 102): 0.37...
likelihood-ratio for (17, 56): 0.62...
</pre></div>
</div>
<p>From which we can conclude that edge <span class="math">\((17, 56)\)</span> is more likely
than <span class="math">\((101, 102)\)</span> to be a missing edge.</p>
<p>The prediction using the non-nested model can be performed in an
entirely analogous fashion.</p>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="inference.html#references" title="Permalink to this headline">¶</a></h2>
<table class="docutils citation" frame="void" id="peixoto-bayesian-2017" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id1">[peixoto-bayesian-2017]</a></td><td>Tiago P. Peixoto, “Bayesian stochastic blockmodeling”,
<a class="reference external" href="https://arxiv.org/abs/1705.10225">arXiv: 1705.10225</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="holland-stochastic-1983" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id3">[holland-stochastic-1983]</a></td><td>Paul W. Holland, Kathryn Blackmond Laskey,
Samuel Leinhardt, “Stochastic blockmodels: First steps”, Social Networks
Volume 5, Issue 2, Pages 109-137 (1983). <a class="reference external" href="https://dx.doi.org/10.1016/0378-8733(83)90021-7">DOI: 10.1016/0378-8733(83)90021-7</a> [<a class="reference external" href="https://sci-hub.tw/10.1016/0378-8733(83)90021-7">sci-hub</a>, <a class="reference external" href="https://scihub22266oqcxt.onion.link/10.1016/0378-8733(83)90021-7">&#64;tor</a>]</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="karrer-stochastic-2011" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id5">[karrer-stochastic-2011]</a></td><td>Brian Karrer, M. E. J. Newman “Stochastic
blockmodels and community structure in networks”, Phys. Rev. E 83,
016107 (2011). <a class="reference external" href="https://dx.doi.org/10.1103/PhysRevE.83.016107">DOI: 10.1103/PhysRevE.83.016107</a> [<a class="reference external" href="https://sci-hub.tw/10.1103/PhysRevE.83.016107">sci-hub</a>, <a class="reference external" href="https://scihub22266oqcxt.onion.link/10.1103/PhysRevE.83.016107">&#64;tor</a>], <a class="reference external" href="https://arxiv.org/abs/1008.3926">arXiv: 1008.3926</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="peixoto-nonparametric-2017" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[peixoto-nonparametric-2017]</td><td><em>(<a class="fn-backref" href="inference.html#id4">1</a>, <a class="fn-backref" href="inference.html#id6">2</a>, <a class="fn-backref" href="inference.html#id12">3</a>, <a class="fn-backref" href="inference.html#id16">4</a>)</em> Tiago P. Peixoto, “Nonparametric
Bayesian inference of the microcanonical stochastic block model”,
Phys. Rev. E 95 012317 (2017). <a class="reference external" href="https://dx.doi.org/10.1103/PhysRevE.95.012317">DOI: 10.1103/PhysRevE.95.012317</a> [<a class="reference external" href="https://sci-hub.tw/10.1103/PhysRevE.95.012317">sci-hub</a>, <a class="reference external" href="https://scihub22266oqcxt.onion.link/10.1103/PhysRevE.95.012317">&#64;tor</a>],
<a class="reference external" href="https://arxiv.org/abs/1610.02703">arXiv: 1610.02703</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="peixoto-parsimonious-2013" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id7">[peixoto-parsimonious-2013]</a></td><td>Tiago P. Peixoto, “Parsimonious module
inference in large networks”, Phys. Rev. Lett. 110, 148701 (2013).
<a class="reference external" href="https://dx.doi.org/10.1103/PhysRevLett.110.148701">DOI: 10.1103/PhysRevLett.110.148701</a> [<a class="reference external" href="https://sci-hub.tw/10.1103/PhysRevLett.110.148701">sci-hub</a>, <a class="reference external" href="https://scihub22266oqcxt.onion.link/10.1103/PhysRevLett.110.148701">&#64;tor</a>], <a class="reference external" href="https://arxiv.org/abs/1212.4794">arXiv: 1212.4794</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="peixoto-hierarchical-2014" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id8">[peixoto-hierarchical-2014]</a></td><td>Tiago P. Peixoto, “Hierarchical block
structures and high-resolution model selection in large networks”,
Phys. Rev. X 4, 011047 (2014). <a class="reference external" href="https://dx.doi.org/10.1103/PhysRevX.4.011047">DOI: 10.1103/PhysRevX.4.011047</a> [<a class="reference external" href="https://sci-hub.tw/10.1103/PhysRevX.4.011047">sci-hub</a>, <a class="reference external" href="https://scihub22266oqcxt.onion.link/10.1103/PhysRevX.4.011047">&#64;tor</a>],
<a class="reference external" href="https://arxiv.org/abs/1310.4377">arXiv: 1310.4377</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="peixoto-model-2016" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id11">[peixoto-model-2016]</a></td><td>Tiago P. Peixoto, “Model selection and hypothesis
testing for large-scale network models with overlapping groups”,
Phys. Rev. X 5, 011033 (2016). <a class="reference external" href="https://dx.doi.org/10.1103/PhysRevX.5.011033">DOI: 10.1103/PhysRevX.5.011033</a> [<a class="reference external" href="https://sci-hub.tw/10.1103/PhysRevX.5.011033">sci-hub</a>, <a class="reference external" href="https://scihub22266oqcxt.onion.link/10.1103/PhysRevX.5.011033">&#64;tor</a>],
<a class="reference external" href="https://arxiv.org/abs/1409.3059">arXiv: 1409.3059</a>.</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="peixoto-inferring-2015" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id28">[peixoto-inferring-2015]</a></td><td>Tiago P. Peixoto, “Inferring the mesoscale
structure of layered, edge-valued and time-varying networks”,
Phys. Rev. E 92, 042807 (2015). <a class="reference external" href="https://dx.doi.org/10.1103/PhysRevE.92.042807">DOI: 10.1103/PhysRevE.92.042807</a> [<a class="reference external" href="https://sci-hub.tw/10.1103/PhysRevE.92.042807">sci-hub</a>, <a class="reference external" href="https://scihub22266oqcxt.onion.link/10.1103/PhysRevE.92.042807">&#64;tor</a>],
<a class="reference external" href="https://arxiv.org/abs/1504.02381">arXiv: 1504.02381</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="aicher-learning-2015" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id18">[aicher-learning-2015]</a></td><td>Christopher Aicher, Abigail Z. Jacobs, and
Aaron Clauset, “Learning Latent Block Structure in Weighted
Networks”, Journal of Complex Networks 3(2). 221-248
(2015). <a class="reference external" href="https://dx.doi.org/10.1093/comnet/cnu026">DOI: 10.1093/comnet/cnu026</a> [<a class="reference external" href="https://sci-hub.tw/10.1093/comnet/cnu026">sci-hub</a>, <a class="reference external" href="https://scihub22266oqcxt.onion.link/10.1093/comnet/cnu026">&#64;tor</a>], <a class="reference external" href="https://arxiv.org/abs/1404.0431">arXiv: 1404.0431</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="peixoto-weighted-2017" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[peixoto-weighted-2017]</td><td><em>(<a class="fn-backref" href="inference.html#id19">1</a>, <a class="fn-backref" href="inference.html#id20">2</a>, <a class="fn-backref" href="inference.html#id22">3</a>, <a class="fn-backref" href="inference.html#id25">4</a>)</em> Tiago P. Peixoto, “Nonparametric weighted
stochastic block models”, <a class="reference external" href="https://arxiv.org/abs/1708.01432">arXiv: 1708.01432</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="peixoto-efficient-2014" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[peixoto-efficient-2014]</td><td><em>(<a class="fn-backref" href="inference.html#id9">1</a>, <a class="fn-backref" href="inference.html#id14">2</a>)</em> Tiago P. Peixoto, “Efficient Monte Carlo and
greedy heuristic for the inference of stochastic block models”, Phys.
Rev. E 89, 012804 (2014). <a class="reference external" href="https://dx.doi.org/10.1103/PhysRevE.89.012804">DOI: 10.1103/PhysRevE.89.012804</a> [<a class="reference external" href="https://sci-hub.tw/10.1103/PhysRevE.89.012804">sci-hub</a>, <a class="reference external" href="https://scihub22266oqcxt.onion.link/10.1103/PhysRevE.89.012804">&#64;tor</a>],
<a class="reference external" href="https://arxiv.org/abs/1310.4378">arXiv: 1310.4378</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="peixoto-reconstructing-2018" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[peixoto-reconstructing-2018]</td><td><em>(<a class="fn-backref" href="inference.html#id30">1</a>, <a class="fn-backref" href="inference.html#id31">2</a>, <a class="fn-backref" href="inference.html#id32">3</a>, <a class="fn-backref" href="inference.html#id34">4</a>)</em> Tiago P. Peixoto, “Reconstructing
networks with unknown and heterogeneous errors”, <a class="reference external" href="https://arxiv.org/abs/1806.07956">arXiv: 1806.07956</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="martin-structural-2015" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id33">[martin-structural-2015]</a></td><td>Travis Martin, Brian Ball, M. E. J. Newman,
“Structural inference for uncertain networks”, Phys. Rev. E 93,
012306 (2016). <a class="reference external" href="https://dx.doi.org/10.1103/PhysRevE.93.012306">DOI: 10.1103/PhysRevE.93.012306</a> [<a class="reference external" href="https://sci-hub.tw/10.1103/PhysRevE.93.012306">sci-hub</a>, <a class="reference external" href="https://scihub22266oqcxt.onion.link/10.1103/PhysRevE.93.012306">&#64;tor</a>], <a class="reference external" href="https://arxiv.org/abs/1506.05490">arXiv: 1506.05490</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="clauset-hierarchical-2008" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id35">[clauset-hierarchical-2008]</a></td><td>Aaron Clauset, Cristopher
Moore, M. E. J. Newman, “Hierarchical structure and the prediction of
missing links in networks”, Nature 453, 98-101 (2008).
<a class="reference external" href="https://dx.doi.org/10.1038/nature06830">DOI: 10.1038/nature06830</a> [<a class="reference external" href="https://sci-hub.tw/10.1038/nature06830">sci-hub</a>, <a class="reference external" href="https://scihub22266oqcxt.onion.link/10.1038/nature06830">&#64;tor</a>]</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="guimera-missing-2009" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id36">[guimera-missing-2009]</a></td><td>Roger Guimerà, Marta Sales-Pardo, “Missing and
spurious interactions and the reconstruction of complex networks”, PNAS
vol. 106 no. 52 (2009). <a class="reference external" href="https://dx.doi.org/10.1073/pnas.0908366106">DOI: 10.1073/pnas.0908366106</a> [<a class="reference external" href="https://sci-hub.tw/10.1073/pnas.0908366106">sci-hub</a>, <a class="reference external" href="https://scihub22266oqcxt.onion.link/10.1073/pnas.0908366106">&#64;tor</a>]</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="valles-catala-consistency-2017" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id37">[valles-catala-consistency-2017]</a></td><td>Toni Vallès-Català,
Tiago P. Peixoto, Roger Guimerà, Marta Sales-Pardo, “On the consistency
between model selection and link prediction in networks”. <a class="reference external" href="https://arxiv.org/abs/1705.07967">arXiv: 1705.07967</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="mezard-information-2009" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id17">[mezard-information-2009]</a></td><td>Marc Mézard, Andrea Montanari, “Information,
Physics, and Computation”, Oxford Univ Press (2009).
<a class="reference external" href="https://dx.doi.org/10.1093/acprof:oso/9780198570837.001.0001">DOI: 10.1093/acprof:oso/9780198570837.001.0001</a> [<a class="reference external" href="https://sci-hub.tw/10.1093/acprof:oso/9780198570837.001.0001">sci-hub</a>, <a class="reference external" href="https://scihub22266oqcxt.onion.link/10.1093/acprof:oso/9780198570837.001.0001">&#64;tor</a>]</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="guimera-modularity-2004" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id2">[guimera-modularity-2004]</a></td><td>Roger Guimerà, Marta Sales-Pardo, and
Luís A. Nunes Amaral, “Modularity from fluctuations in random graphs
and complex networks”, Phys. Rev. E 70, 025101(R) (2004),
<a class="reference external" href="https://dx.doi.org/10.1103/PhysRevE.70.025101">DOI: 10.1103/PhysRevE.70.025101</a> [<a class="reference external" href="https://sci-hub.tw/10.1103/PhysRevE.70.025101">sci-hub</a>, <a class="reference external" href="https://scihub22266oqcxt.onion.link/10.1103/PhysRevE.70.025101">&#64;tor</a>]</td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="hayes-connecting-2006" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id23">[hayes-connecting-2006]</a></td><td>Brian Hayes, “Connecting the dots. can the
tools of graph theory and social-network studies unravel the next big
plot?”, American Scientist, 94(5):400-404, 2006.
<a class="reference external" href="http://www.jstor.org/stable/27858828">http://www.jstor.org/stable/27858828</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="ulanowicz-network-2005" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id26">[ulanowicz-network-2005]</a></td><td>Robert E. Ulanowicz, and
Donald L. DeAngelis. “Network analysis of trophic dynamics in south
florida ecosystems.” US Geological Survey Program on the South
Florida Ecosystem 114 (2005).
<a class="reference external" href="https://fl.water.usgs.gov/PDF_files/ofr99_181_gerould.pdf#page=125">https://fl.water.usgs.gov/PDF_files/ofr99_181_gerould.pdf#page=125</a></td></tr>
</tbody>
</table>
<table class="docutils citation" frame="void" id="read-cultures-1954" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id29">[read-cultures-1954]</a></td><td>Kenneth E. Read, “Cultures of the Central
Highlands, New Guinea”, Southwestern J. of Anthropology,
10(1):1-43 (1954). <a class="reference external" href="https://dx.doi.org/10.1086/soutjanth.10.1.3629074">DOI: 10.1086/soutjanth.10.1.3629074</a> [<a class="reference external" href="https://sci-hub.tw/10.1086/soutjanth.10.1.3629074">sci-hub</a>, <a class="reference external" href="https://scihub22266oqcxt.onion.link/10.1086/soutjanth.10.1.3629074">&#64;tor</a>]</td></tr>
</tbody>
</table>
<p class="rubric">Footnotes</p>
<table class="docutils footnote" frame="void" id="prediction-posterior" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="inference.html#id38">[1]</a></td><td>Note that the posterior of Eq. <a class="reference internal" href="inference.html#equation-posterior-missing">(8)</a>
cannot be used to sample the reconstruction <span class="math">\(\delta \boldsymbol
G\)</span>, as it is not informative of the overall network density
(i.e. absolute number of missing and spurious edges). It can,
however, be used to compare different reconstructions with the same
density.</td></tr>
</tbody>
</table>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../../index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="inference.html#">Inferring modular network structure</a><ul>
<li><a class="reference internal" href="inference.html#background-nonparametric-statistical-inference">Background: Nonparametric statistical inference</a><ul>
<li><a class="reference internal" href="inference.html#minimum-description-length-mdl">Minimum description length (MDL)</a></li>
</ul>
</li>
<li><a class="reference internal" href="inference.html#the-stochastic-block-model-sbm">The stochastic block model (SBM)</a><ul>
<li><a class="reference internal" href="inference.html#the-nested-stochastic-block-model">The nested stochastic block model</a></li>
</ul>
</li>
<li><a class="reference internal" href="inference.html#inferring-the-best-partition">Inferring the best partition</a><ul>
<li><a class="reference internal" href="inference.html#hierarchical-partitions">Hierarchical partitions</a></li>
<li><a class="reference internal" href="inference.html#model-selection">Model selection</a></li>
</ul>
</li>
<li><a class="reference internal" href="inference.html#sampling-from-the-posterior-distribution">Sampling from the posterior distribution</a><ul>
<li><a class="reference internal" href="inference.html#id15">Hierarchical partitions</a></li>
<li><a class="reference internal" href="inference.html#model-class-selection">Model class selection</a></li>
</ul>
</li>
<li><a class="reference internal" href="inference.html#edge-weights-and-covariates">Edge weights and covariates</a><ul>
<li><a class="reference internal" href="inference.html#id24">Model selection</a></li>
<li><a class="reference internal" href="inference.html#posterior-sampling">Posterior sampling</a></li>
</ul>
</li>
<li><a class="reference internal" href="inference.html#layered-networks">Layered networks</a></li>
<li><a class="reference internal" href="inference.html#network-reconstruction">Network reconstruction</a><ul>
<li><a class="reference internal" href="inference.html#measured-networks">Measured networks</a><ul>
<li><a class="reference internal" href="inference.html#heterogeneous-errors">Heterogeneous errors</a></li>
</ul>
</li>
<li><a class="reference internal" href="inference.html#extraneous-error-estimates">Extraneous error estimates</a></li>
<li><a class="reference internal" href="inference.html#edge-prediction-as-binary-classification">Edge prediction as binary classification</a></li>
</ul>
</li>
<li><a class="reference internal" href="inference.html#references">References</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="../index.html"
                        title="previous chapter">Cookbook</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../animation/animation.html"
                        title="next chapter">Animations with graph-tool</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/demos/inference/inference.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../animation/animation.html" title="Animations with graph-tool"
             >next</a> |</li>
        <li class="right" >
          <a href="../index.html" title="Cookbook"
             >previous</a> |</li>
    <li><img src="../../../../img/graph-icon.png" alt="logo" style="margin-right: 5px; margin-bottom:-2px;" /><a href="../../../../index.html">Project Homepage</a> &raquo;</li>
    
        <li class="nav-item nav-item-0"><a href="../../index.html">graph-tool 2.27 documentation</a> &#187;</li>

          <li class="nav-item nav-item-1"><a href="../index.html" >Cookbook</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2018, Tiago de Paula Peixoto &lt;tiago@skewed.de&gt;.
      Last updated on Jun 28, 2018.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.5.
    </div>
  </body>
</html>